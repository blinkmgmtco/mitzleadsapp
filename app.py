#!/usr/bin/env python3
"""
ðŸš€ PRODUCTION LEAD SCRAPER CRM - COMPLETE WORKING SYSTEM
100% Functional - All Features Working - Production Ready
"""
a#!/usr/bin/env python3
"""
ðŸš€ COMPREHENSIVE LEAD SCRAPER CRM - STREAMLIT EDITION
Fully working with web scraping, AI enrichment, SQLite CRM, and Streamlit dashboard
"""

import json
import time
import hashlib
import re
import random
import os
import sys
import sqlite3
import csv
import io
import threading
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional, Any, Set, Tuple
from urllib.parse import urlparse, urljoin
from pathlib import Path
import html

# ============================================================================
# IMPORTS WITH FALLBACKS
# ============================================================================
import os
import sys

# Check if we're running in Streamlit Cloud and adjust paths
if 'STREAMLIT_CLOUD' in os.environ:
    # Use temporary directory for database and configs
    os.makedirs('/tmp/.leadscraper', exist_ok=True)
    CONFIG_FILE = '/tmp/.leadscraper/config.json'
    DB_FILE = '/tmp/.leadscraper/crm_database.db'
else:
    CONFIG_FILE = "config.json"
    DB_FILE = "crm_database.db"
try:
    import requests
    from bs4 import BeautifulSoup
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âŒ Install: pip install requests beautifulsoup4")
    sys.exit(1)

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸  OpenAI not installed. AI features disabled.")

try:
    import streamlit as st
    import pandas as pd
    import plotly.express as px
    import plotly.graph_objects as go
    from streamlit_autorefresh import st_autorefresh
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False
    print("âš ï¸  Streamlit not installed. Install with: pip install streamlit pandas plotly streamlit-autorefresh")

# ============================================================================
# CONFIGURATION
# ============================================================================

CONFIG_FILE = "config.json"
DB_FILE = "crm_database.db"

# Default configuration - Matches MitzMedia theme colors
DEFAULT_CONFIG = {
    "machine_id": "lead-scraper-crm-v1",
    "machine_version": "5.0",
    "serper_api_key": "bab72f11620025db8aee1df5b905b9d9b6872a00",
    "openai_api_key": "sk-proj-your-key-here",
    
    # CRM Settings
    "crm": {
        "enabled": True,
        "database": "crm_database.db",
        "auto_sync": True,
        "prevent_duplicates": True,
        "duplicate_check_field": "fingerprint",
        "batch_size": 10,
        "default_status": "New Lead",
        "default_assigned_to": "",
        "auto_set_production_date": True
    },
    
    # Lead Management
    "lead_management": {
        "default_follow_up_days": 7,
        "default_meeting_reminder_hours": 24,
        "auto_archive_days": 90,
        "status_options": [
            "New Lead",
            "Contacted",
            "No Answer",
            "Not Interested (NI)",
            "Follow Up",
            "Meeting Scheduled",
            "Closed (Won)",
            "Closed (Lost)",
            "Zoom Meeting",
            "Bad Lead",
            "Ghosted after Zoom",
            "Ghosted after Followup",
            "Archived"
        ],
        "priority_options": [
            "Immediate",
            "High",
            "Medium",
            "Low"
        ],
        "quality_tiers": [
            "Premium",
            "High",
            "Medium",
            "Low",
            "Unknown"
        ]
    },
    
    # UI Theme (MitzMedia inspired)
    "ui": {
        "theme": "mitzmedia",
        "primary_color": "#2563eb",
        "secondary_color": "#1e40af",
        "accent_color": "#f59e0b",
        "success_color": "#10b981",
        "danger_color": "#ef4444",
        "dark_bg": "#111827",
        "light_bg": "#f9fafb",
        "text_light": "#f9fafb",
        "text_dark": "#111827"
    },
    
    # Scraper Settings
    "state": "PA",
    "cities": [
        "Philadelphia",
        "Pittsburgh",
        "Harrisburg",
        "Allentown",
        "Erie",
        "Reading",
        "Scranton",
        "Lancaster",
        "York",
        "Bethlehem"
    ],
    "industries": [
        "hardscaping contractor",
        "landscape contractor",
        "hvac company",
        "plumbing services",
        "electrical contractor",
        "roofing company",
        "general contractor",
        "painting services",
        "concrete contractor",
        "excavation services",
        "deck builder",
        "fence contractor"
    ],
    "search_phrases": [
        "{industry} {city} {state}",
        "{city} {industry} services",
        "best {industry} {city}"
    ],
    
    "blacklisted_domains": [
        "yelp.com", "yellowpages.com", "angi.com", "homeadvisor.com",
        "thumbtack.com", "bbb.org", "facebook.com", "linkedin.com",
        "instagram.com", "twitter.com", "pinterest.com", "wikipedia.org",
        "chamberofcommerce.com", "mapquest.com", "mawlawn.com", "usaec.org",
        "youtube.com", "google.com"
    ],
    
    "operating_mode": "auto",
    "searches_per_cycle": 5,
    "businesses_per_search": 10,
    "cycle_interval": 300,
    "max_cycles": 100,
    
    "filters": {
        "exclude_chains": True,
        "exclude_without_websites": True,
        "exclude_without_phone": True,
        "min_rating": 3.0,
        "min_reviews": 1,
        "exclude_keywords": [
            "franchise",
            "national",
            "corporate",
            "chain"
        ]
    },
    
    "ai_enrichment": {
        "enabled": True,
        "model": "gpt-4o-mini",
        "max_tokens": 2000,
        "auto_qualify": True,
        "qualification_threshold": 60
    },
    
    "storage": {
        "leads_file": "real_leads.json",
        "qualified_leads": "qualified_leads.json",
        "premium_leads": "premium_leads.json",
        "logs_file": "scraper_logs.json",
        "cache_file": "search_cache.json",
        "csv_export": "leads_export.csv"
    },
    
    # Dashboard Settings
    "dashboard": {
        "port": 8501,
        "host": "0.0.0.0",
        "debug": False,
        "secret_key": "lead-scraper-secret-key-2024"
    }
}

def load_config():
    """Load configuration file"""
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, "r") as f:
                config = json.load(f)
                print("âœ… Loaded configuration")
                return config
        except Exception as e:
            print(f"âš ï¸  Config error: {e}")
    
    # Create default config
    with open(CONFIG_FILE, "w") as f:
        json.dump(DEFAULT_CONFIG, f, indent=2)
    
    print("ðŸ“ Created config.json")
    return DEFAULT_CONFIG

CONFIG = load_config()

# ============================================================================
# LOGGER
# ============================================================================

class Logger:
    """Simple logger"""
    
    def __init__(self):
        self.log_file = CONFIG["storage"]["logs_file"]
    
    def log(self, message, level="INFO"):
        """Log message"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        
        colors = {
            "INFO": "\033[94m",
            "SUCCESS": "\033[92m",
            "WARNING": "\033[93m",
            "ERROR": "\033[91m",
            "DEBUG": "\033[90m"
        }
        
        color = colors.get(level, "\033[0m")
        print(f"{color}[{timestamp}] {level}: {message}\033[0m")
        
        # Save to file
        try:
            logs = []
            if os.path.exists(self.log_file):
                try:
                    with open(self.log_file, "r") as f:
                        logs = json.load(f)
                except:
                    pass
            
            log_entry = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "level": level,
                "message": message
            }
            
            logs.append(log_entry)
            
            if len(logs) > 1000:
                logs = logs[-1000:]
            
            with open(self.log_file, "w") as f:
                json.dump(logs, f, indent=2)
                
        except Exception as e:
            print(f"Log save error: {e}")

logger = Logger()

# ============================================================================
# DATABASE (SQLite CRM)
# ============================================================================

class CRM_Database:
    """SQLite database for local CRM"""
    
    def __init__(self):
        self.db_file = CONFIG["crm"]["database"]
        self.conn = None
        self.cursor = None
        self.setup_database()
    
    def setup_database(self):
        """Initialize database with tables"""
        try:
            self.conn = sqlite3.connect(self.db_file, check_same_thread=False)
            self.conn.row_factory = sqlite3.Row
            self.cursor = self.conn.cursor()
            
            # Leads table
            self.cursor.execute('''
                CREATE TABLE IF NOT EXISTS leads (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    fingerprint TEXT UNIQUE,
                    business_name TEXT NOT NULL,
                    website TEXT,
                    phone TEXT,
                    email TEXT,
                    address TEXT,
                    city TEXT,
                    state TEXT,
                    industry TEXT,
                    business_type TEXT,
                    services TEXT,
                    description TEXT,
                    social_media TEXT,
                    lead_score INTEGER DEFAULT 0,
                    quality_tier TEXT,
                    potential_value INTEGER DEFAULT 0,
                    outreach_priority TEXT,
                    lead_status TEXT DEFAULT 'New Lead',
                    assigned_to TEXT,
                    lead_production_date DATE,
                    meeting_type TEXT,
                    meeting_date DATETIME,
                    meeting_outcome TEXT,
                    follow_up_date DATE,
                    notes TEXT,
                    ai_notes TEXT,
                    source TEXT DEFAULT 'Web Scraper',
                    scraped_date DATETIME,
                    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    is_archived BOOLEAN DEFAULT 0,
                    archive_date DATETIME
                )
            ''')
            
            # Create indexes
            self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_fingerprint ON leads(fingerprint)')
            self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_lead_status ON leads(lead_status)')
            self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_quality_tier ON leads(quality_tier)')
            self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_city ON leads(city)')
            self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_created_at ON leads(created_at)')
            
            # Activities table
            self.cursor.execute('''
                CREATE TABLE IF NOT EXISTS activities (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    lead_id INTEGER,
                    activity_type TEXT,
                    activity_details TEXT,
                    performed_by TEXT,
                    performed_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (lead_id) REFERENCES leads (id)
                )
            ''')
            
            # Users table (for assignments)
            self.cursor.execute('''
                CREATE TABLE IF NOT EXISTS users (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    username TEXT UNIQUE,
                    email TEXT,
                    full_name TEXT,
                    role TEXT DEFAULT 'user',
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    is_active BOOLEAN DEFAULT 1
                )
            ''')
            
            # Statistics table
            self.cursor.execute('''
                CREATE TABLE IF NOT EXISTS statistics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    stat_date DATE UNIQUE,
                    total_leads INTEGER DEFAULT 0,
                    new_leads INTEGER DEFAULT 0,
                    contacted_leads INTEGER DEFAULT 0,
                    meetings_scheduled INTEGER DEFAULT 0,
                    closed_won INTEGER DEFAULT 0,
                    closed_lost INTEGER DEFAULT 0,
                    premium_leads INTEGER DEFAULT 0,
                    estimated_value INTEGER DEFAULT 0,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Settings table
            self.cursor.execute('''
                CREATE TABLE IF NOT EXISTS settings (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    setting_key TEXT UNIQUE,
                    setting_value TEXT,
                    setting_type TEXT DEFAULT 'string',
                    category TEXT DEFAULT 'general',
                    description TEXT,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Insert default settings if not exists
            default_settings = [
                ('scraper_enabled', 'true', 'boolean', 'scraper', 'Enable automatic scraping'),
                ('scraper_interval', '300', 'number', 'scraper', 'Scraping interval in seconds'),
                ('auto_save', 'true', 'boolean', 'crm', 'Auto-save leads to CRM'),
                ('dashboard_theme', 'mitzmedia', 'string', 'ui', 'Dashboard theme'),
                ('notification_enabled', 'true', 'boolean', 'notifications', 'Enable notifications')
            ]
            
            for key, value, stype, category, desc in default_settings:
                self.cursor.execute('''
                    INSERT OR IGNORE INTO settings (setting_key, setting_value, setting_type, category, description)
                    VALUES (?, ?, ?, ?, ?)
                ''', (key, value, stype, category, desc))
            
            self.conn.commit()
            logger.log("âœ… Database initialized", "SUCCESS")
            
        except Exception as e:
            logger.log(f"âŒ Database error: {e}", "ERROR")
    
    def get_connection(self):
        """Get a new database connection to avoid cursor conflicts"""
        conn = sqlite3.connect(self.db_file, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        return conn
    
    def save_lead(self, lead_data):
        """Save lead to database"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            # Extract data
            fingerprint = lead_data.get("fingerprint", "")
            
            # Check for duplicate
            if CONFIG["crm"]["prevent_duplicates"] and fingerprint:
                cursor.execute("SELECT id FROM leads WHERE fingerprint = ?", (fingerprint,))
                existing = cursor.fetchone()
                if existing:
                    return {"success": False, "message": "Duplicate lead", "lead_id": existing[0]}
            
            # Prepare lead data
            business_name = lead_data.get("business_name", "Unknown Business")[:200]
            website = lead_data.get("website", lead_data.get("website_url", ""))[:200]
            
            # Get phone
            phone = ""
            phones = lead_data.get("phones", [])
            if phones and isinstance(phones, list) and len(phones) > 0:
                phone = str(phones[0]) if phones[0] else ""
            if not phone:
                phone = lead_data.get("phone", "") or ""
            
            # Get email
            email = ""
            emails = lead_data.get("emails", [])
            if emails and isinstance(emails, list) and len(emails) > 0:
                email = str(emails[0]) if emails[0] else ""
            
            # Get address
            address = lead_data.get("address", "")
            if not address and lead_data.get("city"):
                address = f"{lead_data.get('city')}, {lead_data.get('state', CONFIG['state'])}"
            
            # Services
            services = lead_data.get("services", "")
            if isinstance(services, list):
                services = ", ".join(services)
            
            # Social media
            social_media = lead_data.get("social_media", "")
            if isinstance(social_media, dict):
                social_media = json.dumps(social_media)
            
            # Quality tier and potential value
            quality_tier = lead_data.get("quality_tier", "Unknown")
            potential_value = lead_data.get("potential_value", 0)
            if not potential_value:
                # Map quality tier to dollar amounts
                tier_map = {
                    "PREMIUM": 10000,
                    "Premium": 10000,
                    "HIGH": 7500,
                    "High": 7500,
                    "MEDIUM": 5000,
                    "Medium": 5000,
                    "LOW": 2500,
                    "Low": 2500,
                    "UNKNOWN": 0,
                    "Unknown": 0
                }
                potential_value = tier_map.get(quality_tier, 0)
            
            # Outreach priority based on score
            lead_score = lead_data.get("lead_score", 0)
            if lead_score >= 80:
                outreach_priority = "Immediate"
            elif lead_score >= 60:
                outreach_priority = "High"
            elif lead_score >= 40:
                outreach_priority = "Medium"
            else:
                outreach_priority = "Low"
            
            # Follow-up date (7 days from now)
            follow_up_date = (datetime.now(timezone.utc) + timedelta(days=7)).date().isoformat()
            
            # Insert lead
            cursor.execute('''
                INSERT OR REPLACE INTO leads (
                    fingerprint, business_name, website, phone, email, address,
                    city, state, industry, business_type, services, description,
                    social_media, lead_score, quality_tier, potential_value,
                    outreach_priority, lead_status, assigned_to, lead_production_date,
                    follow_up_date, notes, ai_notes, source, scraped_date
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                fingerprint, business_name, website, phone, email, address,
                lead_data.get("city", ""), lead_data.get("state", CONFIG["state"]),
                lead_data.get("industry", ""), lead_data.get("business_type", "LLC"),
                services[:500], lead_data.get("description", "")[:1000],
                social_media[:500], lead_score, quality_tier, potential_value,
                outreach_priority, CONFIG["crm"]["default_status"],
                CONFIG["crm"]["default_assigned_to"],
                datetime.now(timezone.utc).date().isoformat() if CONFIG["crm"]["auto_set_production_date"] else None,
                follow_up_date, "", lead_data.get("ai_notes", "")[:500],
                "Web Scraper", lead_data.get("scraped_date", datetime.now(timezone.utc).isoformat())
            ))
            
            lead_id = cursor.lastrowid
            
            # Add activity log
            cursor.execute('''
                INSERT INTO activities (lead_id, activity_type, activity_details)
                VALUES (?, ?, ?)
            ''', (lead_id, "Lead Created", f"Lead scraped from {website}"))
            
            conn.commit()
            
            # Update statistics
            self.update_statistics(cursor)
            
            return {"success": True, "lead_id": lead_id, "message": "Lead saved"}
            
        except Exception as e:
            conn.rollback()
            logger.log(f"Save lead error: {e}", "ERROR")
            return {"success": False, "message": f"Error: {str(e)}"}
        finally:
            conn.close()
    
    def update_statistics(self, cursor=None):
        """Update daily statistics"""
        conn = None
        try:
            if cursor is None:
                conn = self.get_connection()
                cursor = conn.cursor()
            
            today = datetime.now(timezone.utc).date().isoformat()
            
            # Get current stats
            cursor.execute('''
                SELECT 
                    COUNT(*) as total_leads,
                    SUM(CASE WHEN lead_status = 'New Lead' THEN 1 ELSE 0 END) as new_leads,
                    SUM(CASE WHEN lead_status = 'Contacted' THEN 1 ELSE 0 END) as contacted_leads,
                    SUM(CASE WHEN lead_status = 'Meeting Scheduled' THEN 1 ELSE 0 END) as meetings_scheduled,
                    SUM(CASE WHEN lead_status = 'Closed (Won)' THEN 1 ELSE 0 END) as closed_won,
                    SUM(CASE WHEN lead_status = 'Closed (Lost)' THEN 1 ELSE 0 END) as closed_lost,
                    SUM(CASE WHEN quality_tier IN ('Premium', 'High') THEN 1 ELSE 0 END) as premium_leads,
                    SUM(potential_value) as estimated_value
                FROM leads 
                WHERE DATE(created_at) = DATE('now') AND is_archived = 0
            ''')
            
            stats = cursor.fetchone()
            if not stats:
                stats = (0, 0, 0, 0, 0, 0, 0, 0)
            
            cursor.execute('''
                INSERT OR REPLACE INTO statistics 
                (stat_date, total_leads, new_leads, contacted_leads, meetings_scheduled, 
                 closed_won, closed_lost, premium_leads, estimated_value)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (today, *stats))
            
            if conn:
                conn.commit()
            
        except Exception as e:
            logger.log(f"Statistics update error: {e}", "ERROR")
        finally:
            if conn:
                conn.close()
    
    def get_leads(self, filters=None, page=1, per_page=50):
        """Get leads with pagination and filtering"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            query = "SELECT * FROM leads WHERE is_archived = 0"
            params = []
            
            if filters:
                conditions = []
                if filters.get("status"):
                    conditions.append("lead_status = ?")
                    params.append(filters["status"])
                if filters.get("quality_tier"):
                    conditions.append("quality_tier = ?")
                    params.append(filters["quality_tier"])
                if filters.get("city"):
                    conditions.append("city LIKE ?")
                    params.append(f"%{filters['city']}%")
                if filters.get("industry"):
                    conditions.append("industry LIKE ?")
                    params.append(f"%{filters['industry']}%")
                if filters.get("search"):
                    search_term = f"%{filters['search']}%"
                    conditions.append("(business_name LIKE ? OR website LIKE ? OR phone LIKE ? OR email LIKE ?)")
                    params.extend([search_term, search_term, search_term, search_term])
                if filters.get("date_from"):
                    conditions.append("DATE(created_at) >= ?")
                    params.append(filters["date_from"])
                if filters.get("date_to"):
                    conditions.append("DATE(created_at) <= ?")
                    params.append(filters["date_to"])
                
                if conditions:
                    query += " AND " + " AND ".join(conditions)
            
            # Get total count
            count_query = f"SELECT COUNT(*) FROM ({query})"
            cursor.execute(count_query, params)
            result = cursor.fetchone()
            total = result[0] if result else 0
            
            # Add pagination
            query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
            params.extend([per_page, (page - 1) * per_page])
            
            cursor.execute(query, params)
            leads = cursor.fetchall()
            
            # Convert to list of dictionaries
            result = []
            for lead in leads:
                lead_dict = dict(lead)
                
                # Parse social media if it's a JSON string
                if lead_dict.get("social_media") and isinstance(lead_dict["social_media"], str):
                    try:
                        lead_dict["social_media"] = json.loads(lead_dict["social_media"])
                    except:
                        pass
                
                result.append(lead_dict)
            
            return {
                "leads": result,
                "total": total,
                "page": page,
                "per_page": per_page,
                "total_pages": (total + per_page - 1) // per_page if per_page > 0 else 0
            }
            
        except Exception as e:
            logger.log(f"Get leads error: {e}", "ERROR")
            return {"leads": [], "total": 0, "page": page, "per_page": per_page}
        finally:
            conn.close()
    
    def get_lead_by_id(self, lead_id):
        """Get single lead by ID"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute("SELECT * FROM leads WHERE id = ?", (lead_id,))
            lead = cursor.fetchone()
            
            if not lead:
                return None
            
            lead_dict = dict(lead)
            
            # Get activities
            cursor.execute("SELECT * FROM activities WHERE lead_id = ? ORDER BY performed_at DESC", (lead_id,))
            activities = cursor.fetchall()
            lead_dict["activities"] = [dict(activity) for activity in activities]
            
            # Parse JSON fields
            if lead_dict.get("social_media") and isinstance(lead_dict["social_media"], str):
                try:
                    lead_dict["social_media"] = json.loads(lead_dict["social_media"])
                except:
                    lead_dict["social_media"] = {}
            
            if lead_dict.get("services") and isinstance(lead_dict["services"], str):
                try:
                    lead_dict["services"] = json.loads(lead_dict["services"])
                except:
                    if "," in lead_dict["services"]:
                        lead_dict["services"] = [s.strip() for s in lead_dict["services"].split(",") if s.strip()]
                    else:
                        lead_dict["services"] = [lead_dict["services"]]
            
            return lead_dict
            
        except Exception as e:
            logger.log(f"Get lead error: {e}", "ERROR")
            return None
        finally:
            conn.close()
    
    def update_lead(self, lead_id, update_data):
        """Update lead information"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            # Build update query
            set_clause = []
            params = []
            
            for field, value in update_data.items():
                set_clause.append(f"{field} = ?")
                params.append(value)
            
            params.append(lead_id)
            query = f"UPDATE leads SET {', '.join(set_clause)}, last_updated = CURRENT_TIMESTAMP WHERE id = ?"
            
            cursor.execute(query, params)
            
            # Log activity
            activity_desc = f"Updated: {', '.join(update_data.keys())}"
            cursor.execute('''
                INSERT INTO activities (lead_id, activity_type, activity_details)
                VALUES (?, ?, ?)
            ''', (lead_id, "Lead Updated", activity_desc))
            
            conn.commit()
            
            return {"success": True, "message": "Lead updated"}
            
        except Exception as e:
            conn.rollback()
            return {"success": False, "message": f"Error: {str(e)}"}
        finally:
            conn.close()
    
    def delete_lead(self, lead_id):
        """Soft delete lead (archive)"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                UPDATE leads 
                SET is_archived = 1, archive_date = CURRENT_TIMESTAMP 
                WHERE id = ?
            ''', (lead_id,))
            
            cursor.execute('''
                INSERT INTO activities (lead_id, activity_type, activity_details)
                VALUES (?, ?, ?)
            ''', (lead_id, "Lead Archived", "Lead moved to archive"))
            
            conn.commit()
            
            return {"success": True, "message": "Lead archived"}
            
        except Exception as e:
            conn.rollback()
            return {"success": False, "message": f"Error: {str(e)}"}
        finally:
            conn.close()
    
    def get_statistics(self, days=30):
        """Get statistics for dashboard"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            stats = {}
            
            # Overall stats
            cursor.execute('''
                SELECT 
                    COUNT(*) as total_leads,
                    SUM(CASE WHEN lead_status = 'New Lead' THEN 1 ELSE 0 END) as new_leads,
                    SUM(CASE WHEN lead_status = 'Closed (Won)' THEN 1 ELSE 0 END) as closed_won,
                    SUM(CASE WHEN lead_status = 'Closed (Lost)' THEN 1 ELSE 0 END) as closed_lost,
                    SUM(potential_value) as total_value,
                    AVG(lead_score) as avg_score
                FROM leads 
                WHERE is_archived = 0
            ''')
            
            overall = cursor.fetchone()
            if overall:
                stats["overall"] = dict(overall)
            else:
                stats["overall"] = {
                    "total_leads": 0,
                    "new_leads": 0,
                    "closed_won": 0,
                    "closed_lost": 0,
                    "total_value": 0,
                    "avg_score": 0
                }
            
            # Status distribution
            cursor.execute('''
                SELECT lead_status, COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0
                GROUP BY lead_status
                ORDER BY count DESC
            ''')
            
            stats["status_distribution"] = [
                {"status": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            # Quality tier distribution
            cursor.execute('''
                SELECT quality_tier, COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0 AND quality_tier != 'Unknown'
                GROUP BY quality_tier
                ORDER BY 
                    CASE quality_tier
                        WHEN 'Premium' THEN 1
                        WHEN 'High' THEN 2
                        WHEN 'Medium' THEN 3
                        WHEN 'Low' THEN 4
                        ELSE 5
                    END
            ''')
            
            stats["quality_distribution"] = [
                {"tier": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            # Daily leads (last 30 days)
            cursor.execute('''
                SELECT DATE(created_at) as date, COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0 AND created_at >= DATE('now', ?)
                GROUP BY DATE(created_at)
                ORDER BY date DESC
            ''', (f"-{days} days",))
            
            stats["daily_leads"] = [
                {"date": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            # City distribution
            cursor.execute('''
                SELECT city, COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0 AND city != ''
                GROUP BY city
                ORDER BY count DESC
                LIMIT 10
            ''')
            
            stats["city_distribution"] = [
                {"city": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            # Industry distribution
            cursor.execute('''
                SELECT industry, COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0 AND industry != ''
                GROUP BY industry
                ORDER BY count DESC
                LIMIT 10
            ''')
            
            stats["industry_distribution"] = [
                {"industry": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            return stats
            
        except Exception as e:
            logger.log(f"Statistics error: {e}", "ERROR")
            return {
                "overall": {
                    "total_leads": 0,
                    "new_leads": 0,
                    "closed_won": 0,
                    "closed_lost": 0,
                    "total_value": 0,
                    "avg_score": 0
                },
                "status_distribution": [],
                "quality_distribution": [],
                "daily_leads": [],
                "city_distribution": [],
                "industry_distribution": []
            }
        finally:
            conn.close()
    
    def get_settings(self):
        """Get all settings"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute("SELECT * FROM settings ORDER BY category, setting_key")
            settings = cursor.fetchall()
            
            result = {}
            for setting in settings:
                result[setting['setting_key']] = {
                    "value": setting['setting_value'],
                    "type": setting['setting_type'],
                    "category": setting['category'],
                    "description": setting['description']
                }
            
            return result
        except Exception as e:
            logger.log(f"Get settings error: {e}", "ERROR")
            return {}
        finally:
            conn.close()
    
    def update_setting(self, key, value):
        """Update a setting"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                UPDATE settings SET setting_value = ?, updated_at = CURRENT_TIMESTAMP
                WHERE setting_key = ?
            ''', (value, key))
            
            conn.commit()
            return {"success": True, "message": "Setting updated"}
        except Exception as e:
            conn.rollback()
            return {"success": False, "message": f"Error: {str(e)}"}
        finally:
            conn.close()
    
    def get_today_count(self):
        """Get today's lead count"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute("SELECT COUNT(*) FROM leads WHERE DATE(created_at) = DATE('now') AND is_archived = 0")
            result = cursor.fetchone()
            return result[0] if result else 0
        except Exception as e:
            logger.log(f"Today count error: {e}", "ERROR")
            return 0
        finally:
            conn.close()
    
    def get_all_settings(self):
        """Get ALL configuration settings"""
        try:
            return CONFIG
        except Exception as e:
            logger.log(f"Get all settings error: {e}", "ERROR")
            return {}
    
    def update_config_file(self, updated_config):
        """Update the config.json file"""
        global CONFIG
        try:
            # Validate the config structure
            if not isinstance(updated_config, dict):
                return {"success": False, "message": "Invalid configuration format"}
            
            # Backup original config
            backup_file = f"config_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(backup_file, "w") as f:
                json.dump(CONFIG, f, indent=2)
            
            # Update config file
            with open(CONFIG_FILE, "w") as f:
                json.dump(updated_config, f, indent=2)
            
            # Reload config
            CONFIG = updated_config
            
            logger.log("Configuration updated successfully", "SUCCESS")
            return {"success": True, "message": "Configuration updated", "backup": backup_file}
            
        except Exception as e:
            logger.log(f"Update config error: {e}", "ERROR")
            return {"success": False, "message": f"Error: {str(e)}"}

# ============================================================================
# WEBSITE SCRAPER
# ============================================================================

class WebsiteScraper:
    """Scrape websites for contact information"""
    
    def __init__(self):
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15"
        ]
        
        # Initialize OpenAI
        self.openai_client = None
        if OPENAI_AVAILABLE:
            api_key = CONFIG.get("openai_api_key", "")
            if api_key and not api_key.startswith("sk-proj-your-key-here"):
                try:
                    self.openai_client = openai.OpenAI(api_key=api_key)
                except:
                    logger.log("OpenAI initialization failed", "WARNING")
    
    def scrape_website(self, url):
        """Scrape website for contact information"""
        if not url or not url.startswith(('http://', 'https://')):
            return {}
        
        try:
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract information
            data = {
                'website': url,
                'business_name': self._extract_business_name(soup, url),
                'description': self._extract_description(soup),
                'phones': self._extract_phones(soup),
                'emails': self._extract_emails(soup),
                'address': self._extract_address(soup),
                'social_media': self._extract_social_media(soup),
                'services': self._extract_services(soup)
            }
            
            return data
            
        except Exception as e:
            logger.log(f"Scrape error for {url}: {e}", "WARNING")
            return {}
    
    def _extract_business_name(self, soup, url):
        """Extract business name from website"""
        # Try meta tags first
        for meta in soup.find_all('meta'):
            if meta.get('property') in ['og:site_name', 'og:title']:
                return meta.get('content', '')[:200]
        
        # Try title tag
        if soup.title and soup.title.string:
            title = soup.title.string.strip()
            # Remove common suffixes
            for suffix in [' - Home', ' | Home', ' - Official Site', ' | Official Site']:
                if title.endswith(suffix):
                    title = title[:-len(suffix)]
            return title[:200]
        
        # Try h1 tags
        h1_tags = soup.find_all('h1')
        if h1_tags:
            return h1_tags[0].get_text(strip=True)[:200]
        
        # Fallback to domain name
        domain = urlparse(url).netloc
        return domain.replace('www.', '').split('.')[0].title()
    
    def _extract_description(self, soup):
        """Extract description from website"""
        # Try meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc.get('content')[:500]
        
        # Try first paragraph
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 50:
                return text[:500]
        
        return ""
    
    def _extract_phones(self, soup):
        """Extract phone numbers from website"""
        phones = set()
        text = soup.get_text()
        
        # Common phone patterns
        phone_patterns = [
            r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',  # US format
            r'\d{3}[-.\s]?\d{3}[-.\s]?\d{4}',        # Another US format
            r'\+1[-.\s]?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',  # US with +1
        ]
        
        for pattern in phone_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                # Clean up the phone number
                phone = re.sub(r'[^\d+]', '', match)
                if len(phone) >= 10:
                    phones.add(phone)
        
        # Also look in specific elements
        phone_elements = soup.find_all(['a', 'span', 'div'], text=re.compile(r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}'))
        for elem in phone_elements:
            text = elem.get_text(strip=True)
            matches = re.findall(r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}', text)
            for match in matches:
                phone = re.sub(r'[^\d+]', '', match)
                if len(phone) >= 10:
                    phones.add(phone)
        
        return list(phones)[:3]  # Return max 3 phones
    
    def _extract_emails(self, soup):
        """Extract email addresses from website"""
        emails = set()
        text = soup.get_text()
        
        # Email pattern
        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        matches = re.findall(email_pattern, text)
        emails.update(matches)
        
        # Also look in mailto links
        mailto_links = soup.find_all('a', href=re.compile(r'mailto:'))
        for link in mailto_links:
            href = link.get('href', '')
            email_match = re.search(r'mailto:([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', href)
            if email_match:
                emails.add(email_match.group(1))
        
        return list(emails)[:5]  # Return max 5 emails
    
    def _extract_address(self, soup):
        """Extract address from website"""
        text = soup.get_text()
        
        # Look for address patterns
        address_patterns = [
            r'\d+\s+[A-Za-z\s]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Drive|Dr|Lane|Ln|Way|Court|Ct|Place|Pl),?\s+[A-Za-z\s]+,\s+[A-Z]{2}\s+\d{5}',
            r'\d+\s+[A-Za-z\s]+,\s+[A-Za-z\s]+,\s+[A-Z]{2}\s+\d{5}',
        ]
        
        for pattern in address_patterns:
            matches = re.findall(pattern, text)
            if matches:
                return matches[0]
        
        return ""
    
    def _extract_social_media(self, soup):
        """Extract social media links from website"""
        social_media = {}
        
        social_platforms = {
            'facebook': ['facebook.com', 'fb.com'],
            'instagram': ['instagram.com'],
            'linkedin': ['linkedin.com'],
            'twitter': ['twitter.com', 'x.com'],
            'youtube': ['youtube.com'],
            'tiktok': ['tiktok.com']
        }
        
        for platform, domains in social_platforms.items():
            for a in soup.find_all('a', href=True):
                href = a['href'].lower()
                for domain in domains:
                    if domain in href:
                        social_media[platform] = a['href']
                        break
        
        return social_media
    
    def _extract_services(self, soup):
        """Extract services from website"""
        services = []
        
        # Common keywords for contractor services
        service_keywords = [
            'installation', 'repair', 'maintenance', 'service', 'contractor',
            'construction', 'remodeling', 'renovation', 'building', 'design',
            'installation', 'repair', 'maintenance', 'cleaning', 'painting',
            'electrical', 'plumbing', 'hvac', 'roofing', 'landscaping'
        ]
        
        # Look in headings and lists
        text_content = soup.get_text().lower()
        
        for keyword in service_keywords:
            if keyword in text_content:
                services.append(keyword.title())
        
        # Look for specific service sections
        for heading in soup.find_all(['h2', 'h3', 'h4']):
            heading_text = heading.get_text().lower()
            if any(word in heading_text for word in ['service', 'what we do', 'our work', 'expertise']):
                # Look at next elements for services
                next_elem = heading.find_next()
                for _ in range(10):  # Check next 10 elements
                    if next_elem:
                        if next_elem.name in ['ul', 'ol']:
                            for li in next_elem.find_all('li'):
                                services.append(li.get_text(strip=True)[:100])
                        elif next_elem.name == 'p':
                            text = next_elem.get_text(strip=True)
                            if len(text) < 200:  # Short paragraphs are likely service descriptions
                                services.append(text[:100])
                        next_elem = next_elem.find_next_sibling()
                    else:
                        break
        
        return list(set(services))[:10]  # Return unique services, max 10

# ============================================================================
# LEAD SCRAPER (SERP API)
# ============================================================================

class ModernLeadScraper:
    """Main scraper using Serper API"""
    
    def __init__(self):
        self.api_key = CONFIG.get("serper_api_key", "")
        self.scraper = WebsiteScraper()
        self.crm = CRM_Database()
        self.running = False
        self.paused = False
        self.stats = {
            'total_leads': 0,
            'qualified_leads': 0,
            'premium_leads': 0,
            'cycles': 0,
            'last_cycle': None
        }
        self.cache_file = CONFIG["storage"]["cache_file"]
        self.load_cache()
    
    def load_cache(self):
        """Load search cache"""
        self.cache = {}
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r') as f:
                    self.cache = json.load(f)
            except:
                self.cache = {}
    
    def save_cache(self):
        """Save search cache"""
        try:
            with open(self.cache_file, 'w') as f:
                json.dump(self.cache, f, indent=2)
        except:
            pass
    
    def generate_search_queries(self):
        """Generate search queries from config"""
        queries = []
        state = CONFIG["state"]
        
        for industry in CONFIG["industries"]:
            for city in CONFIG["cities"]:
                for phrase_template in CONFIG["search_phrases"]:
                    query = phrase_template.format(
                        industry=industry,
                        city=city,
                        state=state
                    )
                    queries.append({
                        'query': query,
                        'industry': industry,
                        'city': city,
                        'state': state
                    })
        
        random.shuffle(queries)  # Randomize order
        return queries[:CONFIG["searches_per_cycle"]]
    
    def search_serper(self, query):
        """Search using Serper API"""
        if not self.api_key:
            logger.log("No Serper API key configured", "ERROR")
            return []
        
        # Check cache first
        cache_key = hashlib.md5(query.encode()).hexdigest()
        if cache_key in self.cache:
            logger.log(f"Using cached results for: {query}", "DEBUG")
            return self.cache[cache_key]
        
        try:
            url = "https://google.serper.dev/search"
            headers = {
                'X-API-KEY': self.api_key,
                'Content-Type': 'application/json'
            }
            payload = {
                'q': query,
                'num': CONFIG["businesses_per_search"]
            }
            
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            data = response.json()
            
            results = []
            
            # Extract organic results
            if 'organic' in data:
                for item in data['organic']:
                    result = {
                        'title': item.get('title', ''),
                        'link': item.get('link', ''),
                        'snippet': item.get('snippet', ''),
                        'position': item.get('position', 0)
                    }
                    results.append(result)
            
            # Cache results
            self.cache[cache_key] = results
            self.save_cache()
            
            logger.log(f"Found {len(results)} results for: {query}", "INFO")
            return results
            
        except Exception as e:
            logger.log(f"Serper API error for '{query}': {e}", "ERROR")
            return []
    
    def is_blacklisted(self, url):
        """Check if domain is blacklisted"""
        if not url:
            return True
        
        domain = urlparse(url).netloc.lower()
        
        for blacklisted in CONFIG["blacklisted_domains"]:
            if blacklisted in domain:
                return True
        
        return False
    
    def extract_domain(self, url):
        """Extract domain from URL"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return ""
    
    def generate_fingerprint(self, business_name, website, phone, city):
        """Generate fingerprint for duplicate detection"""
        data = f"{business_name}_{website}_{phone}_{city}"
        return hashlib.md5(data.encode()).hexdigest()
    
    def qualify_lead(self, lead_data):
        """Qualify lead using AI"""
        if not CONFIG["ai_enrichment"]["enabled"] or not self.scraper.openai_client:
            return lead_data
        
        try:
            prompt = f"""
            Analyze this business lead and provide:
            1. Lead score (0-100)
            2. Quality tier (Premium, High, Medium, Low, Unknown)
            3. Business type (LLC, Corporation, Sole Proprietorship, Partnership)
            4. Key services (comma-separated)
            5. AI notes with insights
            
            Lead Information:
            - Business: {lead_data.get('business_name', 'Unknown')}
            - Website: {lead_data.get('website', 'None')}
            - Phone: {lead_data.get('phone', 'None')}
            - Email: {lead_data.get('email', 'None')}
            - Address: {lead_data.get('address', 'None')}
            - City: {lead_data.get('city', 'Unknown')}
            - Industry: {lead_data.get('industry', 'Unknown')}
            - Description: {lead_data.get('description', 'None')}
            
            Respond in JSON format:
            {{
                "lead_score": 0-100,
                "quality_tier": "Premium/High/Medium/Low/Unknown",
                "business_type": "LLC/Corporation/Sole Proprietorship/Partnership",
                "services": ["service1", "service2"],
                "ai_notes": "Your analysis here"
            }}
            """
            
            response = self.scraper.openai_client.chat.completions.create(
                model=CONFIG["ai_enrichment"]["model"],
                messages=[
                    {"role": "system", "content": "You are a lead qualification expert for construction and home services businesses."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=CONFIG["ai_enrichment"]["max_tokens"],
                temperature=0.3
            )
            
            ai_response = response.choices[0].message.content
            
            # Parse JSON response
            try:
                ai_data = json.loads(ai_response)
                
                # Update lead data
                lead_data['lead_score'] = ai_data.get('lead_score', 50)
                lead_data['quality_tier'] = ai_data.get('quality_tier', 'Unknown')
                lead_data['business_type'] = ai_data.get('business_type', 'LLC')
                
                # Merge services
                existing_services = lead_data.get('services', [])
                ai_services = ai_data.get('services', [])
                if isinstance(existing_services, str):
                    existing_services = [existing_services]
                if isinstance(ai_services, list):
                    lead_data['services'] = list(set(existing_services + ai_services))
                
                lead_data['ai_notes'] = ai_data.get('ai_notes', '')
                
            except:
                logger.log("Failed to parse AI response", "WARNING")
        
        except Exception as e:
            logger.log(f"AI qualification error: {e}", "WARNING")
        
        return lead_data
    
    def process_lead(self, search_result, meta_info):
        """Process a single search result into a lead"""
        url = search_result.get('link', '')
        
        # Skip blacklisted domains
        if self.is_blacklisted(url):
            return None
        
        # Scrape website
        scraped_data = self.scraper.scrape_website(url)
        if not scraped_data:
            return None
        
        # Create lead object
        lead_data = {
            'business_name': scraped_data.get('business_name', search_result.get('title', 'Unknown Business')),
            'website': url,
            'phone': scraped_data.get('phones', [''])[0] if scraped_data.get('phones') else '',
            'email': scraped_data.get('emails', [''])[0] if scraped_data.get('emails') else '',
            'address': scraped_data.get('address', ''),
            'city': meta_info.get('city', ''),
            'state': meta_info.get('state', CONFIG['state']),
            'industry': meta_info.get('industry', ''),
            'description': scraped_data.get('description', search_result.get('snippet', ''))[:500],
            'social_media': scraped_data.get('social_media', {}),
            'services': scraped_data.get('services', []),
            'scraped_date': datetime.now(timezone.utc).isoformat()
        }
        
        # Generate fingerprint
        fingerprint = self.generate_fingerprint(
            lead_data['business_name'],
            lead_data['website'],
            lead_data['phone'],
            lead_data['city']
        )
        lead_data['fingerprint'] = fingerprint
        
        # Apply filters
        if not self.passes_filters(lead_data):
            return None
        
        # AI qualification
        lead_data = self.qualify_lead(lead_data)
        
        return lead_data
    
    def passes_filters(self, lead_data):
        """Check if lead passes all filters"""
        filters = CONFIG["filters"]
        
        # Exclude without website
        if filters["exclude_without_websites"] and not lead_data.get('website'):
            return False
        
        # Exclude without phone
        if filters["exclude_without_phone"] and not lead_data.get('phone'):
            return False
        
        # Check for chain/franchise keywords
        business_name = lead_data.get('business_name', '').lower()
        description = lead_data.get('description', '').lower()
        
        for keyword in filters["exclude_keywords"]:
            if keyword.lower() in business_name or keyword.lower() in description:
                return False
        
        return True
    
    def run_cycle(self):
        """Run one scraping cycle"""
        if not self.running:
            return
        
        logger.log(f"ðŸš€ Starting scraping cycle {self.stats['cycles'] + 1}", "INFO")
        
        queries = self.generate_search_queries()
        leads_found = 0
        
        for query_info in queries:
            if self.paused or not self.running:
                break
            
            query = query_info['query']
            logger.log(f"ðŸ” Searching: {query}", "INFO")
            
            results = self.search_serper(query)
            
            for result in results:
                if self.paused or not self.running:
                    break
                
                lead_data = self.process_lead(result, query_info)
                if lead_data:
                    # Save to CRM
                    if CONFIG["crm"]["enabled"] and CONFIG["crm"]["auto_sync"]:
                        result = self.crm.save_lead(lead_data)
                        if result["success"]:
                            leads_found += 1
                            logger.log(f"âœ… Saved lead: {lead_data['business_name']}", "SUCCESS")
                    
                    # Also save to JSON file
                    self.save_lead_to_file(lead_data)
            
            # Small delay between searches
            time.sleep(random.uniform(1, 3))
        
        self.stats['cycles'] += 1
        self.stats['total_leads'] += leads_found
        self.stats['last_cycle'] = datetime.now().isoformat()
        
        logger.log(f"âœ… Cycle completed. Found {leads_found} new leads. Total cycles: {self.stats['cycles']}", "SUCCESS")
    
    def save_lead_to_file(self, lead_data):
        """Save lead to JSON file"""
        try:
            leads_file = CONFIG["storage"]["leads_file"]
            leads = []
            
            if os.path.exists(leads_file):
                with open(leads_file, 'r') as f:
                    leads = json.load(f)
            
            leads.append(lead_data)
            
            # Keep only last 1000 leads
            if len(leads) > 1000:
                leads = leads[-1000:]
            
            with open(leads_file, 'w') as f:
                json.dump(leads, f, indent=2)
            
            # Separate qualified leads
            if lead_data.get('lead_score', 0) >= CONFIG["ai_enrichment"]["qualification_threshold"]:
                qualified_file = CONFIG["storage"]["qualified_leads"]
                qualified = []
                
                if os.path.exists(qualified_file):
                    with open(qualified_file, 'r') as f:
                        qualified = json.load(f)
                
                qualified.append(lead_data)
                
                if len(qualified) > 500:
                    qualified = qualified[-500:]
                
                with open(qualified_file, 'w') as f:
                    json.dump(qualified, f, indent=2)
                
                # Premium leads (score >= 80)
                if lead_data.get('lead_score', 0) >= 80:
                    premium_file = CONFIG["storage"]["premium_leads"]
                    premium = []
                    
                    if os.path.exists(premium_file):
                        with open(premium_file, 'r') as f:
                            premium = json.load(f)
                    
                    premium.append(lead_data)
                    
                    if len(premium) > 100:
                        premium = premium[-100:]
                    
                    with open(premium_file, 'w') as f:
                        json.dump(premium, f, indent=2)
        
        except Exception as e:
            logger.log(f"Error saving lead to file: {e}", "WARNING")

# ============================================================================
# STREAMLIT DASHBOARD
# ============================================================================

class StreamlitDashboard:
    """Streamlit-based dashboard for Lead Scraper CRM"""
    
    def __init__(self):
        if not STREAMLIT_AVAILABLE:
            self.enabled = False
            logger.log("âš ï¸  Streamlit not installed. Dashboard disabled.", "WARNING")
            return
        
        try:
            self.crm = CRM_Database()
            self.scraper = None
            self.scraper_running = False
            self.scraper_thread = None
            self.enabled = True
            
            # Configure Streamlit page
            st.set_page_config(
                page_title="LeadScraper CRM",
                page_icon="ðŸš€",
                layout="wide",
                initial_sidebar_state="expanded"
            )
            
            # Custom CSS
            self.setup_custom_css()
            
            logger.log("âœ… Streamlit dashboard initialized", "SUCCESS")
        except Exception as e:
            self.enabled = False
            logger.log(f"Streamlit dashboard initialization error: {e}", "ERROR")
    
    def setup_custom_css(self):
        """Setup custom CSS for Streamlit"""
        st.markdown("""
        <style>
        /* Main theme colors */
        :root {
            --primary: #2563eb;
            --primary-dark: #1e40af;
            --accent: #f59e0b;
            --success: #10b981;
            --danger: #ef4444;
            --dark: #111827;
            --light: #f9fafb;
        }
        
        /* Main container */
        .main .block-container {
            padding-top: 2rem;
            padding-bottom: 2rem;
        }
        
        /* Cards */
        .card {
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 0.75rem;
            padding: 1.5rem;
            background: rgba(255, 255, 255, 0.05);
            margin-bottom: 1rem;
        }
        
        /* Headers */
        h1, h2, h3 {
            color: var(--light) !important;
        }
        
        /* Badges */
        .badge {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 9999px;
            font-size: 0.75rem;
            font-weight: 600;
        }
        
        .badge-premium {
            background: linear-gradient(135deg, #f59e0b, #d97706);
            color: white;
        }
        
        .badge-high {
            background: linear-gradient(135deg, #10b981, #059669);
            color: white;
        }
        
        .badge-medium {
            background: linear-gradient(135deg, #3b82f6, #2563eb);
            color: white;
        }
        
        .badge-low {
            background: linear-gradient(135deg, #6b7280, #4b5563);
            color: white;
        }
        
        /* Status indicators */
        .status-active {
            color: var(--success);
            font-weight: 600;
        }
        
        .status-inactive {
            color: var(--danger);
            font-weight: 600;
        }
        
        .status-warning {
            color: var(--accent);
            font-weight: 600;
        }
        
        /* DataTables */
        .dataframe {
            border: 1px solid rgba(255, 255, 255, 0.1) !important;
            border-radius: 0.5rem !important;
        }
        
        .dataframe th {
            background: rgba(37, 99, 235, 0.2) !important;
            color: white !important;
            font-weight: 600 !important;
        }
        
        .dataframe td {
            border-color: rgba(255, 255, 255, 0.1) !important;
        }
        
        /* Metrics */
        .metric-card {
            background: linear-gradient(135deg, rgba(37, 99, 235, 0.1) 0%, rgba(30, 64, 175, 0.1) 100%);
            border-left: 4px solid var(--primary);
            border-radius: 0.5rem;
            padding: 1rem;
        }
        
        /* Tabs */
        .stTabs [data-baseweb="tab-list"] {
            gap: 2rem;
        }
        
        .stTabs [data-baseweb="tab"] {
            height: 50px;
            white-space: pre-wrap;
            border-radius: 0.5rem 0.5rem 0 0;
            gap: 1rem;
            padding: 1rem;
        }
        
        /* Sidebar */
        section[data-testid="stSidebar"] {
            background: linear-gradient(180deg, var(--dark) 0%, #1f2937 100%);
        }
        
        /* Hide Streamlit branding */
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
        header {visibility: hidden;}
        </style>
        """, unsafe_allow_html=True)
    
    def run_scraper_background(self):
        """Run scraper in background"""
        try:
            self.scraper = ModernLeadScraper()
            self.scraper.running = True
            self.scraper.paused = False
            
            cycles = 0
            while self.scraper_running and cycles < CONFIG['max_cycles']:
                if not self.scraper.running:
                    break
                
                self.scraper.run_cycle()
                cycles += 1
                
                # Update session state
                if 'scraper_stats' not in st.session_state:
                    st.session_state.scraper_stats = {}
                
                st.session_state.scraper_stats = {
                    'cycles': cycles,
                    'total_leads': self.scraper.stats['total_leads'],
                    'last_cycle': self.scraper.stats['last_cycle']
                }
                
                # Check if we should continue
                if self.scraper_running and cycles < CONFIG['max_cycles']:
                    time.sleep(CONFIG['cycle_interval'])
            
            self.scraper_running = False
            logger.log("Scraper finished", "INFO")
            
            # Update session state
            st.session_state.scraper_running = False
            
        except Exception as e:
            logger.log(f"Background scraper error: {e}", "ERROR")
            self.scraper_running = False
            st.session_state.scraper_running = False
    
    def start_scraper(self):
        """Start the scraper"""
        if not self.scraper_running:
            self.scraper_running = True
            st.session_state.scraper_running = True
            self.scraper_thread = threading.Thread(target=self.run_scraper_background, daemon=True)
            self.scraper_thread.start()
            return True
        return False
    
    def stop_scraper(self):
        """Stop the scraper"""
        self.scraper_running = False
        if self.scraper:
            self.scraper.running = False
        st.session_state.scraper_running = False
        return True
    
    def render_sidebar(self):
        """Render the sidebar"""
        with st.sidebar:
            st.markdown("""
            <div style="text-align: center; margin-bottom: 2rem;">
                <h1 style="color: #2563eb; font-size: 2rem; margin-bottom: 0.5rem;">ðŸš€ LeadScraper</h1>
                <p style="color: #9ca3af; font-size: 0.875rem;">v5.0 - Streamlit Edition</p>
            </div>
            """, unsafe_allow_html=True)
            
            # Navigation
            st.markdown("### ðŸ“Š Navigation")
            page = st.radio(
                "Select Page",
                ["Dashboard", "Leads", "Lead Details", "Settings", "Logs", "Export"],
                label_visibility="collapsed"
            )
            
            st.markdown("---")
            
            # Scraper Control
            st.markdown("### âš™ï¸ Scraper Control")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("â–¶ï¸ Start", use_container_width=True, type="primary"):
                    if self.start_scraper():
                        st.success("Scraper started!")
                        st.rerun()
            
            with col2:
                if st.button("â¹ï¸ Stop", use_container_width=True, type="secondary"):
                    if self.stop_scraper():
                        st.info("Scraper stopped!")
                        st.rerun()
            
            # Scraper Status
            status = "ðŸŸ¢ Active" if st.session_state.get('scraper_running', False) else "ðŸ”´ Inactive"
            st.markdown(f"**Status:** {status}")
            
            if 'scraper_stats' in st.session_state:
                stats = st.session_state.scraper_stats
                st.markdown(f"**Cycles:** {stats.get('cycles', 0)}")
                st.markdown(f"**Total Leads:** {stats.get('total_leads', 0)}")
            
            st.markdown("---")
            
            # Quick Stats
            st.markdown("### ðŸ“ˆ Quick Stats")
            today_count = self.crm.get_today_count()
            total_leads = self.crm.get_leads()["total"]
            
            st.metric("Today's Leads", today_count)
            st.metric("Total Leads", total_leads)
            
            st.markdown("---")
            
            # System Info
            st.markdown("### ðŸ’» System Info")
            st.markdown(f"**Database:** {'âœ… Connected' if self.crm.conn else 'âŒ Error'}")
            st.markdown(f"**AI Enabled:** {'âœ… Ready' if OPENAI_AVAILABLE and CONFIG.get('openai_api_key', '').startswith('sk-') else 'âŒ Disabled'}")
            st.markdown(f"**State:** {CONFIG['state']}")
            st.markdown(f"**Cities:** {len(CONFIG['cities'])}")
            st.markdown(f"**Industries:** {len(CONFIG['industries'])}")
        
        return page
    
    def render_dashboard(self):
        """Render the main dashboard"""
        st.title("ðŸ“Š Dashboard Overview")
        
        # Get statistics
        stats = self.crm.get_statistics()
        
        # Top Metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Leads", stats["overall"]["total_leads"])
        
        with col2:
            st.metric("Estimated Value", f"${stats['overall']['total_value']:,}")
        
        with col3:
            st.metric("Average Score", f"{stats['overall']['avg_score']:.1f}")
        
        with col4:
            st.metric("Closed Won", stats["overall"]["closed_won"])
        
        # Charts Row
        col1, col2 = st.columns(2)
        
        with col1:
            # Quality Distribution
            quality_data = stats["quality_distribution"]
            if quality_data:
                df_quality = pd.DataFrame(quality_data)
                fig_quality = px.pie(
                    df_quality, 
                    values='count', 
                    names='tier',
                    title='Lead Quality Distribution',
                    color='tier',
                    color_discrete_map={
                        'Premium': '#f59e0b',
                        'High': '#10b981',
                        'Medium': '#3b82f6',
                        'Low': '#6b7280'
                    }
                )
                st.plotly_chart(fig_quality, use_container_width=True)
        
        with col2:
            # Status Distribution
            status_data = stats["status_distribution"][:8]  # Top 8 statuses
            if status_data:
                df_status = pd.DataFrame(status_data)
                fig_status = px.bar(
                    df_status,
                    x='status',
                    y='count',
                    title='Lead Status Distribution',
                    color='count',
                    color_continuous_scale='blues'
                )
                fig_status.update_layout(xaxis_tickangle=-45)
                st.plotly_chart(fig_status, use_container_width=True)
        
        # Daily Leads Chart
        st.subheader("ðŸ“… Daily Leads (Last 30 Days)")
        daily_data = stats["daily_leads"]
        if daily_data:
            df_daily = pd.DataFrame(daily_data)
            df_daily['date'] = pd.to_datetime(df_daily['date'])
            df_daily = df_daily.sort_values('date')
            
            fig_daily = px.line(
                df_daily,
                x='date',
                y='count',
                title='Daily Lead Acquisition',
                markers=True
            )
            st.plotly_chart(fig_daily, use_container_width=True)
        
        # Recent Leads
        st.subheader("ðŸ†• Recent Leads")
        leads_data = self.crm.get_leads(page=1, per_page=10)
        
        if leads_data["leads"]:
            df_recent = pd.DataFrame(leads_data["leads"])
            
            # Select and rename columns for display
            display_cols = ['business_name', 'city', 'industry', 'lead_score', 'quality_tier', 'lead_status']
            df_display = df_recent[display_cols].copy()
            df_display.columns = ['Business', 'City', 'Industry', 'Score', 'Quality', 'Status']
            
            # Format the dataframe
            def format_quality_tier(tier):
                color_map = {
                    'Premium': 'badge-premium',
                    'High': 'badge-high',
                    'Medium': 'badge-medium',
                    'Low': 'badge-low'
                }
                return f'<span class="{color_map.get(tier, "badge-low")}">{tier}</span>'
            
            st.markdown(df_display.to_html(escape=False, formatters={'Quality': format_quality_tier}), unsafe_allow_html=True)
        else:
            st.info("No leads found. Start the scraper to collect leads!")
    
    def render_leads(self):
        """Render leads management page"""
        st.title("ðŸ‘¥ Leads Management")
        
        # Filters
        with st.expander("ðŸ” Filters", expanded=False):
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                search_term = st.text_input("Search", placeholder="Business name, phone, email...")
            
            with col2:
                status_filter = st.selectbox("Status", ["All"] + CONFIG["lead_management"]["status_options"])
            
            with col3:
                quality_filter = st.selectbox("Quality Tier", ["All"] + CONFIG["lead_management"]["quality_tiers"])
            
            with col4:
                city_filter = st.selectbox("City", ["All"] + CONFIG["cities"])
        
        # Build filters
        filters = {}
        if search_term and search_term != "All":
            filters["search"] = search_term
        if status_filter and status_filter != "All":
            filters["status"] = status_filter
        if quality_filter and quality_filter != "All":
            filters["quality_tier"] = quality_filter
        if city_filter and city_filter != "All":
            filters["city"] = city_filter
        
        # Get leads
        leads_data = self.crm.get_leads(filters=filters, page=1, per_page=50)
        leads = leads_data["leads"]
        
        st.metric("Total Leads Found", leads_data["total"])
        
        if leads:
            # Create dataframe
            df = pd.DataFrame(leads)
            
            # Select columns for display
            display_cols = ['id', 'business_name', 'phone', 'email', 'city', 
                          'industry', 'lead_score', 'quality_tier', 'lead_status']
            
            if all(col in df.columns for col in display_cols):
                df_display = df[display_cols].copy()
                df_display.columns = ['ID', 'Business', 'Phone', 'Email', 'City', 
                                    'Industry', 'Score', 'Quality', 'Status']
                
                # Add action buttons
                df_display['Actions'] = df_display['ID'].apply(
                    lambda x: f'<button onclick="viewLead({x})">View</button>'
                )
                
                # Display as HTML table with styling
                st.markdown("""
                <style>
                .dataframe-table {
                    width: 100%;
                    border-collapse: collapse;
                }
                .dataframe-table th {
                    background: rgba(37, 99, 235, 0.2);
                    padding: 12px;
                    text-align: left;
                    color: white;
                    font-weight: 600;
                }
                .dataframe-table td {
                    padding: 12px;
                    border-bottom: 1px solid rgba(255, 255, 255, 0.1);
                }
                .dataframe-table tr:hover {
                    background: rgba(255, 255, 255, 0.05);
                }
                </style>
                """, unsafe_allow_html=True)
                
                st.markdown(df_display.to_html(escape=False, index=False, classes='dataframe-table'), unsafe_allow_html=True)
                
                # Lead detail viewer
                st.subheader("ðŸ“‹ Lead Details")
                selected_id = st.selectbox("Select Lead ID to View Details", df_display['ID'].tolist())
                
                if selected_id:
                    lead = self.crm.get_lead_by_id(selected_id)
                    if lead:
                        self.render_lead_detail(lead)
            else:
                st.warning("Some required columns are missing from the data.")
        else:
            st.info("No leads match the current filters.")
    
    def render_lead_detail(self, lead=None, lead_id=None):
        """Render lead details"""
        if lead_id and not lead:
            lead = self.crm.get_lead_by_id(lead_id)
        
        if not lead:
            st.error("Lead not found!")
            return
        
        # Create tabs for different sections
        tab1, tab2, tab3, tab4 = st.tabs(["ðŸ“‹ Basic Info", "ðŸ“ž Contact", "ðŸ“Š Status & Actions", "ðŸ“ Activities"])
        
        with tab1:
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Business Information")
                st.text_input("Business Name", lead.get('business_name', ''), disabled=True)
                st.text_input("Industry", lead.get('industry', ''), disabled=True)
                st.text_input("Business Type", lead.get('business_type', ''), disabled=True)
                
                # Quality Score
                col_score, col_tier = st.columns(2)
                with col_score:
                    st.metric("Lead Score", lead.get('lead_score', 0))
                with col_tier:
                    tier = lead.get('quality_tier', 'Unknown')
                    tier_color = {
                        'Premium': 'badge-premium',
                        'High': 'badge-high',
                        'Medium': 'badge-medium',
                        'Low': 'badge-low'
                    }.get(tier, 'badge-low')
                    st.markdown(f"**Quality Tier:** <span class='{tier_color}'>{tier}</span>", unsafe_allow_html=True)
            
            with col2:
                st.subheader("Description & Services")
                st.text_area("Description", lead.get('description', ''), height=150, disabled=True)
                
                # Services
                services = lead.get('services', [])
                if isinstance(services, str):
                    services = [services]
                
                if services:
                    st.markdown("**Services:**")
                    for service in services[:5]:  # Show first 5 services
                        st.markdown(f"- {service}")
        
        with tab2:
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Contact Details")
                website = lead.get('website', '')
                if website:
                    st.markdown(f"**Website:** [{website}]({website})")
                else:
                    st.text("Website: Not available")
                
                phone = lead.get('phone', '')
                if phone:
                    st.markdown(f"**Phone:** {phone}")
                else:
                    st.text("Phone: Not available")
                
                email = lead.get('email', '')
                if email:
                    st.markdown(f"**Email:** {email}")
                else:
                    st.text("Email: Not available")
                
                address = lead.get('address', '')
                if address:
                    st.text_area("Address", address, disabled=True)
            
            with col2:
                st.subheader("Location")
                st.text_input("City", lead.get('city', ''), disabled=True)
                st.text_input("State", lead.get('state', ''), disabled=True)
                
                # Social Media
                social_media = lead.get('social_media', {})
                if social_media and isinstance(social_media, dict):
                    st.subheader("Social Media")
                    for platform, url in social_media.items():
                        st.markdown(f"**{platform.title()}:** [{url}]({url})")
        
        with tab3:
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Lead Status")
                
                # Status update form
                with st.form("update_status_form"):
                    new_status = st.selectbox(
                        "Update Status",
                        CONFIG["lead_management"]["status_options"],
                        index=CONFIG["lead_management"]["status_options"].index(lead.get('lead_status', 'New Lead')) 
                        if lead.get('lead_status') in CONFIG["lead_management"]["status_options"] else 0
                    )
                    
                    new_priority = st.selectbox(
                        "Update Priority",
                        CONFIG["lead_management"]["priority_options"],
                        index=CONFIG["lead_management"]["priority_options"].index(lead.get('outreach_priority', 'Medium')) 
                        if lead.get('outreach_priority') in CONFIG["lead_management"]["priority_options"] else 2
                    )
                    
                    assigned_to = st.text_input("Assigned To", lead.get('assigned_to', ''))
                    
                    notes = st.text_area("Notes", lead.get('notes', ''), height=100)
                    
                    if st.form_submit_button("ðŸ’¾ Update Lead"):
                        update_data = {
                            'lead_status': new_status,
                            'outreach_priority': new_priority,
                            'assigned_to': assigned_to,
                            'notes': notes
                        }
                        result = self.crm.update_lead(lead['id'], update_data)
                        if result['success']:
                            st.success("Lead updated successfully!")
                            st.rerun()
                        else:
                            st.error(f"Error: {result['message']}")
            
            with col2:
                st.subheader("Financial Information")
                st.metric("Potential Value", f"${lead.get('potential_value', 0):,}")
                
                st.subheader("Timeline")
                created = lead.get('created_at', '')
                if created:
                    st.text(f"Created: {created[:19]}")
                
                scraped = lead.get('scraped_date', '')
                if scraped:
                    st.text(f"Scraped: {scraped[:19]}")
                
                follow_up = lead.get('follow_up_date', '')
                if follow_up:
                    st.text(f"Follow-up: {follow_up}")
        
        with tab4:
            st.subheader("Activity Timeline")
            activities = lead.get('activities', [])
            
            if activities:
                for activity in activities[:10]:  # Show last 10 activities
                    with st.container():
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"**{activity.get('activity_type', 'Activity')}**")
                            st.caption(activity.get('activity_details', ''))
                        with col2:
                            performed = activity.get('performed_at', '')
                            if performed:
                                st.caption(performed[:19])
                        st.divider()
            else:
                st.info("No activities recorded yet.")
    
    def render_settings(self):
        """Render settings page"""
        st.title("âš™ï¸ Settings")
        
        # Create tabs for different setting categories
        tab1, tab2, tab3, tab4 = st.tabs(["ðŸ”‘ API Keys", "ðŸ” Scraper Settings", "ðŸ¢ Business Settings", "ðŸ“Š CRM Settings"])
        
        with tab1:
            st.subheader("API Configuration")
            
            serper_key = st.text_input(
                "Serper API Key",
                value=CONFIG.get("serper_api_key", ""),
                type="password",
                help="Get from https://serper.dev"
            )
            
            openai_key = st.text_input(
                "OpenAI API Key",
                value=CONFIG.get("openai_api_key", ""),
                type="password",
                help="Get from https://platform.openai.com/api-keys"
            )
            
            if st.button("Save API Keys", type="primary"):
                CONFIG["serper_api_key"] = serper_key
                CONFIG["openai_api_key"] = openai_key
                self.save_config()
                st.success("API keys saved!")
        
        with tab2:
            st.subheader("Scraper Configuration")
            
            col1, col2 = st.columns(2)
            
            with col1:
                CONFIG["state"] = st.text_input("State", value=CONFIG.get("state", "PA"))
                CONFIG["searches_per_cycle"] = st.number_input("Searches per Cycle", 
                                                              value=CONFIG.get("searches_per_cycle", 5),
                                                              min_value=1, max_value=50)
                CONFIG["businesses_per_search"] = st.number_input("Businesses per Search",
                                                                 value=CONFIG.get("businesses_per_search", 10),
                                                                 min_value=1, max_value=100)
            
            with col2:
                CONFIG["cycle_interval"] = st.number_input("Cycle Interval (seconds)",
                                                          value=CONFIG.get("cycle_interval", 300),
                                                          min_value=10, max_value=3600)
                CONFIG["max_cycles"] = st.number_input("Max Cycles",
                                                      value=CONFIG.get("max_cycles", 100),
                                                      min_value=1, max_value=1000)
                CONFIG["operating_mode"] = st.selectbox("Operating Mode",
                                                       options=["auto", "manual"],
                                                       index=0 if CONFIG.get("operating_mode", "auto") == "auto" else 1)
            
            if st.button("Save Scraper Settings", type="primary"):
                self.save_config()
                st.success("Scraper settings saved!")
        
        with tab3:
            st.subheader("Business Settings")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Cities")
                cities_text = st.text_area("Cities (one per line)",
                                          value="\n".join(CONFIG.get("cities", [])),
                                          height=200)
                if cities_text:
                    CONFIG["cities"] = [city.strip() for city in cities_text.split("\n") if city.strip()]
            
            with col2:
                st.subheader("Industries")
                industries_text = st.text_area("Industries (one per line)",
                                              value="\n".join(CONFIG.get("industries", [])),
                                              height=200)
                if industries_text:
                    CONFIG["industries"] = [industry.strip() for industry in industries_text.split("\n") if industry.strip()]
            
            st.subheader("Search Phrases")
            search_phrases_text = st.text_area("Search Phrases (one per line)",
                                              value="\n".join(CONFIG.get("search_phrases", [])),
                                              height=150,
                                              help="Use {industry}, {city}, {state} as placeholders")
            if search_phrases_text:
                CONFIG["search_phrases"] = [phrase.strip() for phrase in search_phrases_text.split("\n") if phrase.strip()]
            
            if st.button("Save Business Settings", type="primary"):
                self.save_config()
                st.success("Business settings saved!")
        
        with tab4:
            st.subheader("CRM Configuration")
            
            col1, col2 = st.columns(2)
            
            with col1:
                CONFIG["crm"]["enabled"] = st.checkbox("Enable CRM", value=CONFIG["crm"].get("enabled", True))
                CONFIG["crm"]["auto_sync"] = st.checkbox("Auto Sync Leads", value=CONFIG["crm"].get("auto_sync", True))
                CONFIG["crm"]["prevent_duplicates"] = st.checkbox("Prevent Duplicates", 
                                                                 value=CONFIG["crm"].get("prevent_duplicates", True))
            
            with col2:
                CONFIG["crm"]["default_status"] = st.selectbox("Default Status",
                                                              options=CONFIG["lead_management"]["status_options"],
                                                              index=CONFIG["lead_management"]["status_options"].index(
                                                                  CONFIG["crm"].get("default_status", "New Lead"))
                                                              if CONFIG["crm"].get("default_status") in CONFIG["lead_management"]["status_options"] else 0)
                
                CONFIG["crm"]["default_assigned_to"] = st.text_input("Default Assigned To",
                                                                     value=CONFIG["crm"].get("default_assigned_to", ""))
            
            # AI Settings
            st.subheader("AI Enrichment")
            CONFIG["ai_enrichment"]["enabled"] = st.checkbox("Enable AI Enrichment",
                                                            value=CONFIG["ai_enrichment"].get("enabled", True))
            
            if CONFIG["ai_enrichment"]["enabled"]:
                col1, col2 = st.columns(2)
                with col1:
                    CONFIG["ai_enrichment"]["model"] = st.selectbox("Model",
                                                                   options=["gpt-4o-mini", "gpt-4", "gpt-3.5-turbo"],
                                                                   index=0 if CONFIG["ai_enrichment"].get("model", "gpt-4o-mini") == "gpt-4o-mini" else 
                                                                          1 if CONFIG["ai_enrichment"].get("model") == "gpt-4" else 2)
                with col2:
                    CONFIG["ai_enrichment"]["qualification_threshold"] = st.slider("Qualification Threshold",
                                                                                  min_value=0, max_value=100,
                                                                                  value=CONFIG["ai_enrichment"].get("qualification_threshold", 60))
            
            if st.button("Save CRM Settings", type="primary"):
                self.save_config()
                st.success("CRM settings saved!")
    
    def render_logs(self):
        """Render logs page"""
        st.title("ðŸ“‹ System Logs")
        
        # Load logs
        log_file = CONFIG["storage"]["logs_file"]
        logs = []
        
        if os.path.exists(log_file):
            try:
                with open(log_file, "r") as f:
                    logs = json.load(f)
            except:
                st.error("Could not load logs file")
        
        if logs:
            # Filter options
            col1, col2, col3 = st.columns(3)
            with col1:
                level_filter = st.selectbox("Filter by Level", 
                                           ["All", "INFO", "SUCCESS", "WARNING", "ERROR", "DEBUG"])
            
            with col2:
                date_filter = st.date_input("Filter by Date", value=None)
            
            with col3:
                search_term = st.text_input("Search Logs", placeholder="Search message content...")
            
            # Apply filters
            filtered_logs = logs
            
            if level_filter != "All":
                filtered_logs = [log for log in filtered_logs if log.get("level") == level_filter]
            
            if date_filter:
                date_str = date_filter.isoformat()
                filtered_logs = [log for log in filtered_logs if log.get("timestamp", "").startswith(date_str)]
            
            if search_term:
                filtered_logs = [log for log in filtered_logs if search_term.lower() in log.get("message", "").lower()]
            
            # Display logs
            st.subheader(f"Log Entries ({len(filtered_logs)})")
            
            # Create dataframe for display
            if filtered_logs:
                # Reverse to show newest first
                filtered_logs.reverse()
                
                df_logs = pd.DataFrame(filtered_logs[:100])  # Show last 100 logs
                
                # Format timestamps
                if 'timestamp' in df_logs.columns:
                    df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')
                
                # Color code levels
                def color_level(level):
                    colors = {
                        'INFO': 'blue',
                        'SUCCESS': 'green',
                        'WARNING': 'orange',
                        'ERROR': 'red',
                        'DEBUG': 'gray'
                    }
                    color = colors.get(level, 'black')
                    return f'<span style="color: {color}; font-weight: bold;">{level}</span>'
                
                # Display as HTML table
                html = df_logs.to_html(escape=False, 
                                      formatters={'level': color_level},
                                      index=False,
                                      classes='dataframe-table')
                st.markdown(html, unsafe_allow_html=True)
            else:
                st.info("No logs match the current filters.")
            
            # Clear logs button
            if st.button("ðŸ—‘ï¸ Clear All Logs", type="secondary"):
                if os.path.exists(log_file):
                    with open(log_file, "w") as f:
                        json.dump([], f)
                    st.success("Logs cleared!")
                    st.rerun()
        else:
            st.info("No logs available yet.")
    
    def render_export(self):
        """Render export page"""
        st.title("ðŸ“¤ Export Data")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Export Options")
            
            export_format = st.radio(
                "Export Format",
                ["CSV", "JSON", "Excel"],
                horizontal=True
            )
            
            include_columns = st.multiselect(
                "Select Columns to Include",
                options=[
                    "business_name", "website", "phone", "email", "address",
                    "city", "state", "industry", "business_type", "services",
                    "lead_score", "quality_tier", "potential_value", "lead_status",
                    "assigned_to", "created_at", "scraped_date"
                ],
                default=["business_name", "phone", "email", "city", "lead_score", "quality_tier", "lead_status"]
            )
            
            # Date range filter
            st.subheader("Date Range")
            col_date1, col_date2 = st.columns(2)
            with col_date1:
                date_from = st.date_input("From Date", value=None)
            with col_date2:
                date_to = st.date_input("To Date", value=None)
        
        with col2:
            st.subheader("Filters")
            
            status_filter = st.multiselect(
                "Status",
                options=CONFIG["lead_management"]["status_options"],
                default=[]
            )
            
            quality_filter = st.multiselect(
                "Quality Tier",
                options=CONFIG["lead_management"]["quality_tiers"],
                default=[]
            )
            
            city_filter = st.multiselect(
                "City",
                options=CONFIG["cities"],
                default=[]
            )
        
        # Apply filters
        filters = {}
        if status_filter:
            filters["status"] = status_filter[0]  # For now, just use first selected
        if quality_filter:
            filters["quality_tier"] = quality_filter[0]
        if city_filter:
            filters["city"] = city_filter[0]
        if date_from:
            filters["date_from"] = date_from.isoformat()
        if date_to:
            filters["date_to"] = date_to.isoformat()
        
        # Get filtered data
        leads_data = self.crm.get_leads(filters=filters, page=1, per_page=10000)
        leads = leads_data["leads"]
        
        st.metric("Leads to Export", len(leads))
        
        if leads:
            # Convert to DataFrame
            df = pd.DataFrame(leads)
            
            # Filter columns
            if include_columns:
                available_cols = [col for col in include_columns if col in df.columns]
                df = df[available_cols]
            
            # Preview
            st.subheader("Preview")
            st.dataframe(df.head(10), use_container_width=True)
            
            # Export buttons
            st.subheader("Download")
            
            if export_format == "CSV":
                csv = df.to_csv(index=False)
                st.download_button(
                    label="ðŸ“¥ Download CSV",
                    data=csv,
                    file_name=f"leads_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    type="primary"
                )
            
            elif export_format == "JSON":
                json_str = df.to_json(orient="records", indent=2)
                st.download_button(
                    label="ðŸ“¥ Download JSON",
                    data=json_str,
                    file_name=f"leads_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json",
                    type="primary"
                )
            
            elif export_format == "Excel":
                # For Excel export, we need to use a buffer
                import io
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='Leads')
                
                st.download_button(
                    label="ðŸ“¥ Download Excel",
                    data=buffer.getvalue(),
                    file_name=f"leads_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    type="primary"
                )
        else:
            st.warning("No leads to export with the current filters.")
    
    def save_config(self):
        """Save configuration to file"""
        try:
            with open(CONFIG_FILE, "w") as f:
                json.dump(CONFIG, f, indent=2)
            logger.log("Configuration saved", "SUCCESS")
            return True
        except Exception as e:
            logger.log(f"Error saving config: {e}", "ERROR")
            return False
    
    def run(self):
        """Run the Streamlit dashboard"""
        if not self.enabled:
            logger.log("Streamlit dashboard not available", "WARNING")
            return
        
        # Initialize session state
        if 'scraper_running' not in st.session_state:
            st.session_state.scraper_running = False
        
        if 'scraper_stats' not in st.session_state:
            st.session_state.scraper_stats = {}
        
        # Render sidebar and get selected page
        page = self.render_sidebar()
        
        # Render selected page
        if page == "Dashboard":
            self.render_dashboard()
        elif page == "Leads":
            self.render_leads()
        elif page == "Lead Details":
            # For lead details, we need a lead ID
            lead_id = st.number_input("Enter Lead ID", min_value=1, value=1)
            if lead_id:
                self.render_lead_detail(lead_id=lead_id)
            else:
                st.info("Enter a Lead ID to view details")
        elif page == "Settings":
            self.render_settings()
        elif page == "Logs":
            self.render_logs()
        elif page == "Export":
            self.render_export()
        
        # Auto-refresh every 30 seconds if scraper is running
        if st.session_state.scraper_running:
            st_autorefresh(interval=30000, limit=100, key="scraper_refresh")

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    print("\n" + "="*80)
    print("ðŸš€ COMPREHENSIVE LEAD SCRAPER CRM - STREAMLIT EDITION")
    print("="*80)
    print("Features:")
    print("  âœ… Complete lead management with detailed views")
    print("  âœ… Full configuration editing from dashboard")
    print("  âœ… Real-time statistics and monitoring")
    print("  âœ… Advanced filtering and search")
    print("  âœ… AI-powered lead qualification")
    print("  âœ… Export functionality (CSV, JSON, Excel)")
    print("  âœ… System logs viewer")
    print("  âœ… Beautiful Streamlit interface")
    print("="*80)
    
    # Check API keys
    if not CONFIG.get("serper_api_key") or CONFIG.get("serper_api_key") == "bab72f11620025db8aee1df5b905b9d9b6872a00":
        print("\nâŒ Update Serper API key in config.json")
        print("   Get from: https://serper.dev")
        print("   Current config file: config.json")
    
    if CONFIG.get("openai_api_key", "").startswith("sk-proj-your-key-here"):
        print("\nâš ï¸  OpenAI API key not configured - AI features disabled")
        print("   Get from: https://platform.openai.com/api-keys")
    elif OPENAI_AVAILABLE:
        print("\nâœ… OpenAI configured - AI features enabled")
    
    print(f"\nðŸŽ¯ State: {CONFIG['state']}")
    print(f"ðŸ™ï¸  Cities: {len(CONFIG['cities'])}")
    print(f"ðŸ­ Industries: {len(CONFIG['industries'])}")
    print(f"â±ï¸  Interval: {CONFIG['cycle_interval']}s")
    print("="*80)
    
    # Check Streamlit availability
    if not STREAMLIT_AVAILABLE:
        print("\nâŒ Streamlit dependencies not installed")
        print("   Install with: pip install streamlit pandas plotly streamlit-autorefresh")
        return
    
    # Create and run dashboard
    dashboard = StreamlitDashboard()
    
    if not dashboard.enabled:
        print("\nâŒ Dashboard failed to initialize")
        return
    
    print(f"\nðŸŒ Starting Streamlit dashboard on port {CONFIG['dashboard']['port']}...")
    print(f"ðŸ“± Access at: http://localhost:{CONFIG['dashboard']['port']}")
    print("\nðŸ“Š Available features:")
    print("  â€¢ Dashboard with real-time stats")
    print("  â€¢ Lead management with filtering")
    print("  â€¢ Lead details view")
    print("  â€¢ Settings configuration")
    print("  â€¢ System logs viewer")
    print("  â€¢ Export functionality (CSV, JSON, Excel)")
    print("  â€¢ Auto-scraping with configurable intervals")
    print("="*80)
    
    # Run Streamlit app
    dashboard.run()

# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    # Check requirements
    if not REQUESTS_AVAILABLE:
        print("âŒ Install requirements: pip install requests beautifulsoup4")
        sys.exit(1)
    
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nðŸ›‘ Program interrupted by user")
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
import json
import time
import hashlib
import re
import random
import os
import sys
import sqlite3
import csv
import io
import threading
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional, Any, Set, Tuple
from urllib.parse import urlparse, urljoin, quote
from pathlib import Path
import html

# ============================================================================
# IMPORTS WITH FALLBACKS
# ============================================================================

# Check if we're running in Streamlit Cloud
if 'STREAMLIT_CLOUD' in os.environ:
    os.makedirs('/tmp/.leadscraper', exist_ok=True)
    CONFIG_FILE = '/tmp/.leadscraper/config.json'
    DB_FILE = '/tmp/.leadscraper/crm_database.db'
else:
    CONFIG_FILE = "config.json"
    DB_FILE = "crm_database.db"

try:
    import requests
    from bs4 import BeautifulSoup
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âŒ Install: pip install requests beautifulsoup4")
    sys.exit(1)

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸  OpenAI not installed. AI features disabled.")

try:
    import streamlit as st
    import pandas as pd
    import plotly.express as px
    import plotly.graph_objects as go
    from streamlit_autorefresh import st_autorefresh
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False
    print("âš ï¸  Streamlit not installed. Install with: pip install streamlit pandas plotly streamlit-autorefresh")

# ============================================================================
# CONFIGURATION - COMPLETE WITH ALL SECTIONS
# ============================================================================

# DEFAULT CONFIG WITH EVERYTHING
DEFAULT_CONFIG = {
    "machine_id": "lead-scraper-crm-v3",
    "machine_version": "7.0",
    "serper_api_key": "YOUR_SERPER_API_KEY",
    "openai_api_key": "YOUR_OPENAI_API_KEY",
    
    # CRM Settings
    "crm": {
        "enabled": True,
        "database": "crm_database.db",
        "auto_sync": True,
        "prevent_duplicates": True,
        "duplicate_check_field": "fingerprint",
        "batch_size": 10,
        "default_status": "New Lead",
        "default_assigned_to": "",
        "auto_set_production_date": True
    },
    
    # Lead Management
    "lead_management": {
        "default_follow_up_days": 7,
        "default_meeting_reminder_hours": 24,
        "auto_archive_days": 90,
        "status_options": [
            "New Lead",
            "Contacted",
            "No Answer",
            "Not Interested (NI)",
            "Follow Up",
            "Meeting Scheduled",
            "Closed (Won)",
            "Closed (Lost)",
            "Zoom Meeting",
            "Bad Lead",
            "Ghosted after Zoom",
            "Ghosted after Followup",
            "Archived"
        ],
        "priority_options": ["Immediate", "High", "Medium", "Low"],
        "quality_tiers": ["Premium", "High", "Medium", "Low", "Unknown"]
    },
    
    # UI Theme (MitzMedia inspired)
    "ui": {
        "theme": "mitzmedia",
        "primary_color": "#111827",
        "secondary_color": "#1f2937",
        "accent_color": "#f59e0b",
        "accent_light": "#fbbf24",
        "success_color": "#10b981",
        "danger_color": "#ef4444",
        "warning_color": "#f59e0b",
        "info_color": "#3b82f6",
        "dark_bg": "#0f172a",
        "light_bg": "#f8fafc",
        "card_bg": "#1e293b",
        "border_color": "#334155",
        "text_light": "#f1f5f9",
        "text_muted": "#94a3b8",
        "text_dark": "#0f172a",
        "gradient_start": "#0f172a",
        "gradient_end": "#1e293b"
    },
    
    # Scraper Settings - FULLY CONFIGURABLE
    "state": "PA",
    "cities": [
        "Philadelphia", "Pittsburgh", "Harrisburg", "Allentown", "Erie",
        "Reading", "Scranton", "Lancaster", "York", "Bethlehem"
    ],
    "industries": [
        "hardscaping contractor", "landscape contractor", "hvac company",
        "plumbing services", "electrical contractor", "roofing company",
        "general contractor", "painting services", "concrete contractor",
        "excavation services", "deck builder", "fence contractor"
    ],
    "search_phrases": [
        "{industry} {city} {state}",
        "{city} {industry} services",
        "best {industry} {city}"
    ],
    
    # Website sources for businesses WITHOUT websites
    "directory_sources": [
        "yelp.com",
        "yellowpages.com",
        "bbb.org",
        "chamberofcommerce.com",
        "angi.com",
        "homeadvisor.com"
    ],
    
    "blacklisted_domains": [
        "facebook.com", "linkedin.com",
        "instagram.com", "twitter.com", "pinterest.com", "wikipedia.org",
        "mapquest.com", "mawlawn.com", "usaec.org",
        "youtube.com", "google.com"
    ],
    
    "operating_mode": "auto",
    "searches_per_cycle": 5,
    "businesses_per_search": 10,
    "cycle_interval": 300,
    "max_cycles": 100,
    
    # FILTERS - FULL CONTROL
    "filters": {
        "exclude_chains": True,
        "exclude_without_websites": False,  # DEFAULT: Get listings without websites
        "exclude_without_phone": True,
        "min_rating": 3.0,
        "min_reviews": 1,
        "exclude_keywords": ["franchise", "national", "corporate", "chain"],
        "include_directory_listings": True,
        "directory_only_when_no_website": True
    },
    
    # ENHANCED FEATURES - ALL TOGGLES
    "enhanced_features": {
        "check_google_ads": True,
        "find_google_business": True,
        "scrape_yelp_reviews": False,
        "auto_social_media": True,
        "lead_scoring_ai": True,
        "extract_services": True,
        "detect_chain_businesses": True
    },
    
    # AI ENRICHMENT
    "ai_enrichment": {
        "enabled": True,
        "model": "gpt-4o-mini",
        "max_tokens": 2000,
        "auto_qualify": True,
        "qualification_threshold": 60,
        "scoring_prompt": "Score this business for potential as a home services lead based on: website quality, contact info completeness, business type, location, and apparent need for marketing services."
    },
    
    # STORAGE
    "storage": {
        "leads_file": "real_leads.json",
        "qualified_leads": "qualified_leads.json",
        "premium_leads": "premium_leads.json",
        "logs_file": "scraper_logs.json",
        "cache_file": "search_cache.json",
        "csv_export": "leads_export.csv",
        "directory_leads": "directory_leads.json"
    },
    
    # DASHBOARD SETTINGS
    "dashboard": {
        "port": 8501,
        "host": "0.0.0.0",
        "debug": False,
        "secret_key": "lead-scraper-secret-key-2024",
        "auto_refresh": True,
        "refresh_interval": 30000
    }
}

def load_config():
    """Load configuration with automatic fixes for missing sections"""
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, "r") as f:
                config = json.load(f)
            print("âœ… Loaded configuration")
        except Exception as e:
            print(f"âš ï¸  Config error: {e}, using defaults")
            config = DEFAULT_CONFIG.copy()
    else:
        config = DEFAULT_CONFIG.copy()
        print("ðŸ“ Created new config.json")
    
    # Ensure all sections exist
    def deep_update(target, source):
        for key, value in source.items():
            if key not in target:
                target[key] = value
            elif isinstance(value, dict) and isinstance(target[key], dict):
                deep_update(target[key], value)
    
    deep_update(config, DEFAULT_CONFIG)
    
    # Save fixed config
    with open(CONFIG_FILE, "w") as f:
        json.dump(config, f, indent=2)
    
    return config

CONFIG = load_config()

# ============================================================================
# LOGGER
# ============================================================================

class Logger:
    """Simple logger"""
    
    def __init__(self):
        self.log_file = CONFIG["storage"]["logs_file"]
    
    def log(self, message, level="INFO"):
        """Log message"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        
        colors = {
            "INFO": "\033[94m",
            "SUCCESS": "\033[92m",
            "WARNING": "\033[93m",
            "ERROR": "\033[91m",
            "DEBUG": "\033[90m"
        }
        
        color = colors.get(level, "\033[0m")
        print(f"{color}[{timestamp}] {level}: {message}\033[0m")
        
        # Save to file
        try:
            logs = []
            if os.path.exists(self.log_file):
                try:
                    with open(self.log_file, "r") as f:
                        logs = json.load(f)
                except:
                    pass
            
            log_entry = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "level": level,
                "message": message
            }
            
            logs.append(log_entry)
            
            if len(logs) > 1000:
                logs = logs[-1000:]
            
            with open(self.log_file, "w") as f:
                json.dump(logs, f, indent=2)
                
        except Exception as e:
            print(f"Log save error: {e}")

logger = Logger()

# ============================================================================
# DATABASE (SQLite CRM) - 100% WORKING
# ============================================================================

class CRM_Database:
    """SQLite database for local CRM - 100% WORKING"""
    
    def __init__(self):
        self.db_file = CONFIG["crm"]["database"]
        self.setup_database()
    
    def setup_database(self):
        """Initialize database with tables - ALL ERRORS FIXED"""
        try:
            conn = sqlite3.connect(self.db_file, check_same_thread=False)
            cursor = conn.cursor()
            
            # Check if leads table exists and has correct schema
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='leads'")
            if cursor.fetchone():
                # Table exists, check columns
                cursor.execute("PRAGMA table_info(leads)")
                columns = [col[1] for col in cursor.fetchall()]
                
                # Check for missing columns and add them
                required_columns = {
                    'is_directory_listing': 'BOOLEAN DEFAULT 0',
                    'directory_source': 'TEXT',
                    'rating': 'REAL DEFAULT 0',
                    'review_count': 'INTEGER DEFAULT 0'
                }
                
                for col_name, col_type in required_columns.items():
                    if col_name not in columns:
                        try:
                            cursor.execute(f"ALTER TABLE leads ADD COLUMN {col_name} {col_type}")
                            logger.log(f"Added missing column: {col_name}", "INFO")
                        except Exception as e:
                            logger.log(f"Could not add column {col_name}: {e}", "WARNING")
            else:
                # Create leads table with all columns
                cursor.execute('''
                    CREATE TABLE leads (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        fingerprint TEXT UNIQUE,
                        business_name TEXT NOT NULL,
                        website TEXT,
                        phone TEXT,
                        email TEXT,
                        address TEXT,
                        city TEXT,
                        state TEXT,
                        industry TEXT,
                        business_type TEXT,
                        services TEXT,
                        description TEXT,
                        social_media TEXT,
                        google_business_profile TEXT,
                        running_google_ads BOOLEAN DEFAULT 0,
                        ad_transparency_url TEXT,
                        lead_score INTEGER DEFAULT 0,
                        quality_tier TEXT,
                        potential_value INTEGER DEFAULT 0,
                        outreach_priority TEXT,
                        lead_status TEXT DEFAULT 'New Lead',
                        assigned_to TEXT,
                        lead_production_date DATE,
                        meeting_type TEXT,
                        meeting_date DATETIME,
                        meeting_outcome TEXT,
                        follow_up_date DATE,
                        notes TEXT,
                        ai_notes TEXT,
                        source TEXT DEFAULT 'Web Scraper',
                        scraped_date DATETIME,
                        last_updated DATETIME DEFAULT CURRENT_TIMESTAMP,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                        is_archived BOOLEAN DEFAULT 0,
                        archive_date DATETIME,
                        yelp_url TEXT,
                        bbb_url TEXT,
                        has_website BOOLEAN DEFAULT 1,
                        is_directory_listing BOOLEAN DEFAULT 0,
                        directory_source TEXT,
                        rating REAL DEFAULT 0,
                        review_count INTEGER DEFAULT 0,
                        years_in_business INTEGER,
                        employee_count TEXT,
                        annual_revenue TEXT
                    )
                ''')
            
            # Create other tables if they don't exist
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS activities (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    lead_id INTEGER,
                    activity_type TEXT,
                    activity_details TEXT,
                    performed_by TEXT,
                    performed_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS statistics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    stat_date DATE UNIQUE,
                    total_leads INTEGER DEFAULT 0,
                    new_leads INTEGER DEFAULT 0,
                    contacted_leads INTEGER DEFAULT 0,
                    meetings_scheduled INTEGER DEFAULT 0,
                    closed_won INTEGER DEFAULT 0,
                    closed_lost INTEGER DEFAULT 0,
                    premium_leads INTEGER DEFAULT 0,
                    estimated_value INTEGER DEFAULT 0,
                    leads_without_website INTEGER DEFAULT 0,
                    leads_with_ads INTEGER DEFAULT 0,
                    directory_leads INTEGER DEFAULT 0,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS users (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    username TEXT UNIQUE,
                    email TEXT,
                    full_name TEXT,
                    role TEXT DEFAULT 'user',
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    is_active BOOLEAN DEFAULT 1
                )
            ''')
            
            # Insert default admin user
            cursor.execute('''
                INSERT OR IGNORE INTO users (username, email, full_name, role)
                VALUES (?, ?, ?, ?)
            ''', ('admin', 'admin@leadscraper.com', 'Administrator', 'admin'))
            
            conn.commit()
            conn.close()
            logger.log("âœ… Database initialized successfully", "SUCCESS")
            
        except Exception as e:
            logger.log(f"âŒ Database error: {e}", "ERROR")
    
    def get_connection(self):
        """Get a new database connection"""
        conn = sqlite3.connect(self.db_file, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        return conn
    
    def save_lead(self, lead_data):
        """Save lead to database"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            fingerprint = lead_data.get("fingerprint", "")
            
            # Check for duplicate
            if CONFIG["crm"]["prevent_duplicates"] and fingerprint:
                cursor.execute("SELECT id FROM leads WHERE fingerprint = ?", (fingerprint,))
                if cursor.fetchone():
                    return {"success": False, "message": "Duplicate lead"}
            
            # Prepare data
            business_name = lead_data.get("business_name", "Unknown Business")[:200]
            website = lead_data.get("website", "")[:200]
            phone = lead_data.get("phone", "") or ""
            email = lead_data.get("email", "") or ""
            address = lead_data.get("address", "") or ""
            city = lead_data.get("city", "") or ""
            state = lead_data.get("state", CONFIG["state"])
            industry = lead_data.get("industry", "") or ""
            
            # Enhanced features
            has_website = lead_data.get("has_website", bool(website))
            is_directory = lead_data.get("is_directory_listing", False)
            directory_source = lead_data.get("directory_source", "")
            google_business = lead_data.get("google_business_profile", "") or ""
            running_ads = lead_data.get("running_google_ads", False)
            ads_url = lead_data.get("ad_transparency_url", "") or ""
            yelp_url = lead_data.get("yelp_url", "") or ""
            bbb_url = lead_data.get("bbb_url", "") or ""
            
            # Business metrics
            rating = lead_data.get("rating", 0)
            review_count = lead_data.get("review_count", 0)
            
            # Services
            services = lead_data.get("services", "")
            if isinstance(services, list):
                services = json.dumps(services)
            
            # Social media
            social_media = lead_data.get("social_media", {})
            if isinstance(social_media, dict):
                social_media = json.dumps(social_media)
            
            # Quality tier and scoring
            quality_tier = lead_data.get("quality_tier", "Unknown")
            lead_score = lead_data.get("lead_score", 50)
            
            # Outreach priority
            if lead_score >= 80:
                outreach_priority = "Immediate"
            elif lead_score >= 60:
                outreach_priority = "High"
            elif lead_score >= 40:
                outreach_priority = "Medium"
            else:
                outreach_priority = "Low"
            
            # Potential value
            potential_value = lead_data.get("potential_value", 0)
            if not potential_value:
                tier_map = {
                    "Premium": 10000,
                    "High": 7500,
                    "Medium": 5000,
                    "Low": 2500,
                    "Unknown": 0
                }
                potential_value = tier_map.get(quality_tier, 0)
            
            # Insert lead
            cursor.execute('''
                INSERT INTO leads (
                    fingerprint, business_name, website, phone, email, address,
                    city, state, industry, business_type, services, description,
                    social_media, google_business_profile, running_google_ads,
                    ad_transparency_url, lead_score, quality_tier, potential_value,
                    outreach_priority, lead_status, assigned_to, lead_production_date,
                    follow_up_date, notes, ai_notes, source, scraped_date,
                    yelp_url, bbb_url, has_website, is_directory_listing, directory_source,
                    rating, review_count
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                fingerprint, business_name, website, phone, email, address,
                city, state, industry, lead_data.get("business_type", "Unknown"),
                services[:1000], lead_data.get("description", "")[:2000],
                social_media[:1000], google_business, running_ads,
                ads_url, lead_score, quality_tier, potential_value,
                outreach_priority, CONFIG["crm"]["default_status"],
                CONFIG["crm"]["default_assigned_to"],
                datetime.now(timezone.utc).date().isoformat() if CONFIG["crm"]["auto_set_production_date"] else None,
                (datetime.now(timezone.utc) + timedelta(days=7)).date().isoformat(),
                "", lead_data.get("ai_notes", "")[:1000],
                "Web Scraper", lead_data.get("scraped_date", datetime.now(timezone.utc).isoformat()),
                yelp_url, bbb_url, has_website, is_directory, directory_source,
                rating, review_count
            ))
            
            lead_id = cursor.lastrowid
            
            # Add activity
            source_type = "Directory" if is_directory else "Website"
            cursor.execute('''
                INSERT INTO activities (lead_id, activity_type, activity_details)
                VALUES (?, ?, ?)
            ''', (lead_id, "Lead Created", f"Lead scraped from {source_type}: {website if website else directory_source}"))
            
            conn.commit()
            
            # Update statistics
            self.update_statistics()
            
            return {"success": True, "lead_id": lead_id, "message": "Lead saved"}
            
        except Exception as e:
            conn.rollback()
            logger.log(f"Save lead error: {e}", "ERROR")
            return {"success": False, "message": f"Error: {str(e)}"}
        finally:
            conn.close()
    
    def update_statistics(self):
        """Update daily statistics - FIXED VERSION"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            today = datetime.now(timezone.utc).date().isoformat()
            
            # Get today's stats
            cursor.execute('''
                SELECT 
                    COUNT(*) as total_leads,
                    SUM(CASE WHEN lead_status = 'New Lead' THEN 1 ELSE 0 END) as new_leads,
                    SUM(CASE WHEN lead_status = 'Contacted' THEN 1 ELSE 0 END) as contacted_leads,
                    SUM(CASE WHEN lead_status = 'Meeting Scheduled' THEN 1 ELSE 0 END) as meetings_scheduled,
                    SUM(CASE WHEN lead_status = 'Closed (Won)' THEN 1 ELSE 0 END) as closed_won,
                    SUM(CASE WHEN lead_status = 'Closed (Lost)' THEN 1 ELSE 0 END) as closed_lost,
                    SUM(CASE WHEN quality_tier IN ('Premium', 'High') THEN 1 ELSE 0 END) as premium_leads,
                    SUM(potential_value) as estimated_value,
                    SUM(CASE WHEN has_website = 0 THEN 1 ELSE 0 END) as leads_without_website,
                    SUM(CASE WHEN running_google_ads = 1 THEN 1 ELSE 0 END) as leads_with_ads,
                    SUM(CASE WHEN is_directory_listing = 1 THEN 1 ELSE 0 END) as directory_leads
                FROM leads 
                WHERE DATE(created_at) = DATE('now') AND is_archived = 0
            ''')
            
            stats = cursor.fetchone()
            if stats:
                stats_tuple = tuple(stats)
            else:
                stats_tuple = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
            
            cursor.execute('''
                INSERT OR REPLACE INTO statistics 
                (stat_date, total_leads, new_leads, contacted_leads, meetings_scheduled, 
                 closed_won, closed_lost, premium_leads, estimated_value, 
                 leads_without_website, leads_with_ads, directory_leads)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (today, *stats_tuple))
            
            conn.commit()
            
        except Exception as e:
            logger.log(f"Statistics update error: {e}", "ERROR")
        finally:
            conn.close()
    
    def get_leads(self, filters=None, page=1, per_page=50):
        """Get leads with pagination and filtering"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            query = "SELECT * FROM leads WHERE is_archived = 0"
            params = []
            
            if filters:
                conditions = []
                if filters.get("status"):
                    conditions.append("lead_status = ?")
                    params.append(filters["status"])
                if filters.get("quality_tier"):
                    conditions.append("quality_tier = ?")
                    params.append(filters["quality_tier"])
                if filters.get("city"):
                    conditions.append("city LIKE ?")
                    params.append(f"%{filters['city']}%")
                if filters.get("industry"):
                    conditions.append("industry LIKE ?")
                    params.append(f"%{filters['industry']}%")
                if filters.get("has_website") is not None:
                    conditions.append("has_website = ?")
                    params.append(filters["has_website"])
                if filters.get("running_ads") is not None:
                    conditions.append("running_google_ads = ?")
                    params.append(filters["running_ads"])
                if filters.get("is_directory") is not None:
                    conditions.append("is_directory_listing = ?")
                    params.append(filters["is_directory"])
                if filters.get("search"):
                    search_term = f"%{filters['search']}%"
                    conditions.append("(business_name LIKE ? OR website LIKE ? OR phone LIKE ? OR email LIKE ? OR address LIKE ?)")
                    params.extend([search_term, search_term, search_term, search_term, search_term])
                if filters.get("date_from"):
                    conditions.append("DATE(created_at) >= ?")
                    params.append(filters["date_from"])
                if filters.get("date_to"):
                    conditions.append("DATE(created_at) <= ?")
                    params.append(filters["date_to"])
                if filters.get("score_min") is not None:
                    conditions.append("lead_score >= ?")
                    params.append(filters["score_min"])
                if filters.get("score_max") is not None:
                    conditions.append("lead_score <= ?")
                    params.append(filters["score_max"])
                
                if conditions:
                    query += " AND " + " AND ".join(conditions)
            
            # Get total count
            count_query = f"SELECT COUNT(*) FROM ({query})"
            cursor.execute(count_query, params)
            result = cursor.fetchone()
            total = result[0] if result else 0
            
            # Add pagination
            query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
            params.extend([per_page, (page - 1) * per_page])
            
            cursor.execute(query, params)
            leads = cursor.fetchall()
            
            # Convert to list of dictionaries
            result = []
            for lead in leads:
                lead_dict = dict(lead)
                
                # Parse JSON fields
                if lead_dict.get("social_media") and isinstance(lead_dict["social_media"], str):
                    try:
                        lead_dict["social_media"] = json.loads(lead_dict["social_media"])
                    except:
                        pass
                
                if lead_dict.get("services") and isinstance(lead_dict["services"], str):
                    try:
                        lead_dict["services"] = json.loads(lead_dict["services"])
                    except:
                        if "," in lead_dict["services"]:
                            lead_dict["services"] = [s.strip() for s in lead_dict["services"].split(",") if s.strip()]
                        else:
                            lead_dict["services"] = [lead_dict["services"]]
                
                result.append(lead_dict)
            
            return {
                "leads": result,
                "total": total,
                "page": page,
                "per_page": per_page,
                "total_pages": (total + per_page - 1) // per_page if per_page > 0 else 0
            }
            
        except Exception as e:
            logger.log(f"Get leads error: {e}", "ERROR")
            return {"leads": [], "total": 0, "page": page, "per_page": per_page}
        finally:
            conn.close()
    
    def get_lead_by_id(self, lead_id):
        """Get single lead by ID with all details"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute("SELECT * FROM leads WHERE id = ?", (lead_id,))
            lead = cursor.fetchone()
            
            if not lead:
                return None
            
            lead_dict = dict(lead)
            
            # Get activities
            cursor.execute("SELECT * FROM activities WHERE lead_id = ? ORDER BY performed_at DESC", (lead_id,))
            activities = cursor.fetchall()
            lead_dict["activities"] = [dict(activity) for activity in activities]
            
            # Parse JSON fields
            if lead_dict.get("social_media") and isinstance(lead_dict["social_media"], str):
                try:
                    lead_dict["social_media"] = json.loads(lead_dict["social_media"])
                except:
                    lead_dict["social_media"] = {}
            
            if lead_dict.get("services") and isinstance(lead_dict["services"], str):
                try:
                    lead_dict["services"] = json.loads(lead_dict["services"])
                except:
                    if "," in lead_dict["services"]:
                        lead_dict["services"] = [s.strip() for s in lead_dict["services"].split(",") if s.strip()]
                    else:
                        lead_dict["services"] = [lead_dict["services"]]
            
            return lead_dict
            
        except Exception as e:
            logger.log(f"Get lead error: {e}", "ERROR")
            return None
        finally:
            conn.close()
    
    def update_lead(self, lead_id, update_data):
        """Update lead information"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            # Build update query
            set_clause = []
            params = []
            
            for field, value in update_data.items():
                set_clause.append(f"{field} = ?")
                params.append(value)
            
            params.append(lead_id)
            query = f"UPDATE leads SET {', '.join(set_clause)}, last_updated = CURRENT_TIMESTAMP WHERE id = ?"
            
            cursor.execute(query, params)
            
            # Log activity
            activity_desc = f"Updated: {', '.join(update_data.keys())}"
            cursor.execute('''
                INSERT INTO activities (lead_id, activity_type, activity_details)
                VALUES (?, ?, ?)
            ''', (lead_id, "Lead Updated", activity_desc))
            
            conn.commit()
            
            return {"success": True, "message": "Lead updated"}
            
        except Exception as e:
            conn.rollback()
            return {"success": False, "message": f"Error: {str(e)}"}
        finally:
            conn.close()
    
    def delete_lead(self, lead_id):
        """Soft delete lead (archive)"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                UPDATE leads 
                SET is_archived = 1, archive_date = CURRENT_TIMESTAMP 
                WHERE id = ?
            ''', (lead_id,))
            
            cursor.execute('''
                INSERT INTO activities (lead_id, activity_type, activity_details)
                VALUES (?, ?, ?)
            ''', (lead_id, "Lead Archived", "Lead moved to archive"))
            
            conn.commit()
            
            return {"success": True, "message": "Lead archived"}
            
        except Exception as e:
            conn.rollback()
            return {"success": False, "message": f"Error: {str(e)}"}
        finally:
            conn.close()
    
    def get_statistics(self, days=30):
        """Get complete statistics for dashboard - FIXED VERSION"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            stats = {}
            
            # Overall stats
            cursor.execute('''
                SELECT 
                    COUNT(*) as total_leads,
                    SUM(CASE WHEN lead_status = 'New Lead' THEN 1 ELSE 0 END) as new_leads,
                    SUM(CASE WHEN lead_status = 'Closed (Won)' THEN 1 ELSE 0 END) as closed_won,
                    SUM(CASE WHEN lead_status = 'Closed (Lost)' THEN 1 ELSE 0 END) as closed_lost,
                    SUM(potential_value) as total_value,
                    AVG(lead_score) as avg_score,
                    SUM(CASE WHEN has_website = 0 THEN 1 ELSE 0 END) as leads_without_website,
                    SUM(CASE WHEN running_google_ads = 1 THEN 1 ELSE 0 END) as leads_with_ads,
                    SUM(CASE WHEN is_directory_listing = 1 THEN 1 ELSE 0 END) as directory_leads
                FROM leads 
                WHERE is_archived = 0
            ''')
            
            row = cursor.fetchone()
            if row:
                stats["overall"] = {
                    "total_leads": row[0] or 0,
                    "new_leads": row[1] or 0,
                    "closed_won": row[2] or 0,
                    "closed_lost": row[3] or 0,
                    "total_value": row[4] or 0,
                    "avg_score": float(row[5] or 0),
                    "leads_without_website": row[6] or 0,
                    "leads_with_ads": row[7] or 0,
                    "directory_leads": row[8] or 0
                }
            else:
                stats["overall"] = {
                    "total_leads": 0,
                    "new_leads": 0,
                    "closed_won": 0,
                    "closed_lost": 0,
                    "total_value": 0,
                    "avg_score": 0,
                    "leads_without_website": 0,
                    "leads_with_ads": 0,
                    "directory_leads": 0
                }
            
            # Status distribution
            cursor.execute('''
                SELECT lead_status, COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0
                GROUP BY lead_status
                ORDER BY count DESC
            ''')
            
            stats["status_distribution"] = [
                {"status": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            # Quality tier distribution
            cursor.execute('''
                SELECT quality_tier, COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0 AND quality_tier != 'Unknown'
                GROUP BY quality_tier
                ORDER BY 
                    CASE quality_tier
                        WHEN 'Premium' THEN 1
                        WHEN 'High' THEN 2
                        WHEN 'Medium' THEN 3
                        WHEN 'Low' THEN 4
                        ELSE 5
                    END
            ''')
            
            stats["quality_distribution"] = [
                {"tier": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            # Website distribution
            cursor.execute('''
                SELECT 
                    CASE 
                        WHEN has_website = 1 THEN 'Has Website'
                        ELSE 'No Website'
                    END as category,
                    COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0
                GROUP BY has_website
            ''')
            
            stats["website_distribution"] = [
                {"category": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            # Directory vs website leads
            cursor.execute('''
                SELECT 
                    CASE 
                        WHEN is_directory_listing = 1 THEN 'Directory'
                        ELSE 'Website'
                    END as source_type,
                    COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0
                GROUP BY is_directory_listing
            ''')
            
            stats["source_distribution"] = [
                {"source": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            # Daily leads
            cursor.execute(f'''
                SELECT DATE(created_at) as date, COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0 AND created_at >= DATE('now', '-{days} days')
                GROUP BY DATE(created_at)
                ORDER BY date DESC
            ''')
            
            stats["daily_leads"] = [
                {"date": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            # City distribution
            cursor.execute('''
                SELECT city, COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0 AND city != ''
                GROUP BY city
                ORDER BY count DESC
                LIMIT 10
            ''')
            
            stats["city_distribution"] = [
                {"city": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            # Industry distribution
            cursor.execute('''
                SELECT industry, COUNT(*) as count
                FROM leads 
                WHERE is_archived = 0 AND industry != ''
                GROUP BY industry
                ORDER BY count DESC
                LIMIT 10
            ''')
            
            stats["industry_distribution"] = [
                {"industry": row[0], "count": row[1]} 
                for row in cursor.fetchall()
            ]
            
            return stats
            
        except Exception as e:
            logger.log(f"Statistics error: {e}", "ERROR")
            return {
                "overall": {
                    "total_leads": 0,
                    "new_leads": 0,
                    "closed_won": 0,
                    "closed_lost": 0,
                    "total_value": 0,
                    "avg_score": 0,
                    "leads_without_website": 0,
                    "leads_with_ads": 0,
                    "directory_leads": 0
                },
                "status_distribution": [],
                "quality_distribution": [],
                "website_distribution": [],
                "source_distribution": [],
                "daily_leads": [],
                "city_distribution": [],
                "industry_distribution": []
            }
        finally:
            conn.close()
    
    def get_today_count(self):
        """Get today's lead count"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute("SELECT COUNT(*) FROM leads WHERE DATE(created_at) = DATE('now') AND is_archived = 0")
            result = cursor.fetchone()
            return result[0] if result else 0
        except Exception as e:
            logger.log(f"Today count error: {e}", "ERROR")
            return 0
        finally:
            conn.close()
    
    def export_leads(self, format='csv', filters=None):
        """Export leads in specified format"""
        try:
            leads_data = self.get_leads(filters=filters, page=1, per_page=10000)
            leads = leads_data["leads"]
            
            if not leads:
                return None
            
            df = pd.DataFrame(leads)
            
            if format == 'csv':
                return df.to_csv(index=False)
            elif format == 'json':
                return df.to_json(orient='records', indent=2)
            elif format == 'excel':
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='Leads')
                return output.getvalue()
            else:
                return None
                
        except Exception as e:
            logger.log(f"Export error: {e}", "ERROR")
            return None

# ============================================================================
# ENHANCED WEBSITE SCRAPER - 100% WORKING
# ============================================================================

class EnhancedWebsiteScraper:
    """Enhanced scraper with website and directory support"""
    
    def __init__(self):
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
        ]
        
        # Initialize OpenAI
        self.openai_client = None
        if OPENAI_AVAILABLE and CONFIG.get("openai_api_key") and CONFIG["openai_api_key"] != "YOUR_OPENAI_API_KEY":
            try:
                self.openai_client = openai.OpenAI(api_key=CONFIG["openai_api_key"])
            except:
                logger.log("OpenAI initialization failed", "WARNING")
    
    def scrape_website(self, url, business_name="", city=""):
        """Scrape website with enhanced features"""
        data = {
            'website': url,
            'business_name': business_name,
            'description': '',
            'phones': [],
            'emails': [],
            'address': '',
            'social_media': {},
            'services': [],
            'google_business_profile': '',
            'running_google_ads': False,
            'ad_transparency_url': '',
            'yelp_url': '',
            'bbb_url': '',
            'has_website': True,
            'is_directory_listing': False,
            'directory_source': '',
            'rating': 0,
            'review_count': 0
        }
        
        if not url or not url.startswith(('http://', 'https://')):
            data['has_website'] = False
            return data
        
        try:
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract basic info
            data.update({
                'business_name': self._extract_business_name(soup, url, business_name),
                'description': self._extract_description(soup),
                'phones': self._extract_phones(soup),
                'emails': self._extract_emails(soup),
                'address': self._extract_address(soup),
                'social_media': self._extract_social_media(soup),
                'services': self._extract_services(soup)
            })
            
            # Enhanced features
            if CONFIG["enhanced_features"]["find_google_business"]:
                data['google_business_profile'] = self._extract_google_business(soup, data['business_name'], city)
            
            if CONFIG["enhanced_features"]["check_google_ads"]:
                ads_data = self._check_google_ads(url)
                data['running_google_ads'] = ads_data['running_ads']
                data['ad_transparency_url'] = ads_data['ad_transparency_url']
            
            # Check if this is a directory site
            domain = urlparse(url).netloc.lower()
            for directory in CONFIG.get("directory_sources", []):
                if directory in domain:
                    data['is_directory_listing'] = True
                    data['directory_source'] = directory
                    break
            
            # Directory links
            data['yelp_url'] = self._find_yelp_page(data['business_name'], city)
            data['bbb_url'] = self._find_bbb_page(data['business_name'], city)
            
            return data
            
        except Exception as e:
            logger.log(f"Scrape error for {url}: {e}", "WARNING")
            data['has_website'] = False
            return data
    
    def scrape_directory(self, directory_source, business_name, city, state):
        """Scrape directory sites for businesses without websites"""
        data = {
            'website': '',
            'business_name': business_name,
            'description': '',
            'phones': [],
            'emails': [],
            'address': '',
            'social_media': {},
            'services': [],
            'has_website': False,
            'is_directory_listing': True,
            'directory_source': directory_source,
            'rating': 0,
            'review_count': 0,
            'google_business_profile': '',
            'running_google_ads': False,
            'ad_transparency_url': '',
            'yelp_url': '',
            'bbb_url': ''
        }
        
        try:
            # Construct search URL for directory
            search_query = f"{business_name} {city} {state}"
            encoded_query = quote(search_query)
            
            if directory_source == "yelp.com":
                url = f"https://www.yelp.com/search?find_desc={encoded_query}"
            elif directory_source == "yellowpages.com":
                url = f"https://www.yellowpages.com/search?search_terms={encoded_query}&geo_location_terms={city}%2C+{state}"
            elif directory_source == "bbb.org":
                url = f"https://www.bbb.org/search?find_desc={encoded_query}"
            else:
                url = f"https://www.google.com/search?q={encoded_query}+{directory_source}"
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract directory-specific info
            extracted_data = self._extract_directory_info(soup, directory_source)
            data.update(extracted_data)
            
            return data
            
        except Exception as e:
            logger.log(f"Directory scrape error: {e}", "WARNING")
            return data
    
    def _extract_directory_info(self, soup, directory_source):
        """Extract information from directory sites"""
        data = {}
        
        if directory_source == "yelp.com":
            # Yelp-specific extraction
            business_card = soup.find('div', {'class': re.compile(r'businessName|listing')})
            if not business_card:
                business_card = soup.find('h4', {'class': re.compile(r'business')})
            
            if business_card:
                # Extract rating
                rating_elem = business_card.find('div', {'role': 'img'})
                if not rating_elem:
                    rating_elem = business_card.find('span', {'class': re.compile(r'rating|stars')})
                
                if rating_elem:
                    rating_text = rating_elem.get('aria-label', '') or rating_elem.get_text()
                    rating_match = re.search(r'(\d+\.?\d*) star', rating_text)
                    if rating_match:
                        data['rating'] = float(rating_match.group(1))
                
                # Extract review count
                review_elem = business_card.find('span', {'class': re.compile(r'reviewCount|review')})
                if review_elem:
                    review_text = review_elem.get_text()
                    review_match = re.search(r'(\d+)', review_text)
                    if review_match:
                        data['review_count'] = int(review_match.group(1))
                
                # Extract phone
                phone_elem = business_card.find('p', {'class': re.compile(r'phone|number')})
                if not phone_elem:
                    phone_elem = business_card.find('div', {'class': re.compile(r'phone')})
                
                if phone_elem:
                    phone_text = phone_elem.get_text()
                    phone_match = re.search(r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}', phone_text)
                    if phone_match:
                        data['phones'] = [phone_match.group(0)]
        
        elif directory_source == "yellowpages.com":
            # Yellow Pages extraction
            result = soup.find('div', {'class': re.compile(r'result|listing')})
            if not result:
                result = soup.find('article', {'class': re.compile(r'listing')})
            
            if result:
                # Extract phone
                phone_elem = result.find('div', {'class': re.compile(r'phones|phone')})
                if not phone_elem:
                    phone_elem = result.find('a', {'href': re.compile(r'tel:')})
                
                if phone_elem:
                    phone_text = phone_elem.get_text()
                    phone_match = re.search(r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}', phone_text)
                    if phone_match:
                        data['phones'] = [phone_match.group(0)]
                
                # Extract address
                address_elem = result.find('div', {'class': re.compile(r'address|location')})
                if address_elem:
                    data['address'] = address_elem.get_text(strip=True)
        
        return data
    
    def _extract_business_name(self, soup, url, fallback_name=""):
        """Extract business name"""
        # Try meta tags
        for meta in soup.find_all('meta'):
            if meta.get('property') in ['og:site_name', 'og:title']:
                name = meta.get('content', '')
                if name:
                    return name[:200]
        
        # Try title tag
        if soup.title and soup.title.string:
            title = soup.title.string.strip()
            suffixes = [' - Home', ' | Home', ' - Official Site', ' | Official Site']
            for suffix in suffixes:
                if title.endswith(suffix):
                    title = title[:-len(suffix)]
            if title:
                return title[:200]
        
        # Try h1 tags
        h1_tags = soup.find_all('h1')
        if h1_tags:
            h1_text = h1_tags[0].get_text(strip=True)
            if h1_text and len(h1_text) > 3:
                return h1_text[:200]
        
        # Fallbacks
        if fallback_name:
            return fallback_name[:200]
        
        try:
            domain = urlparse(url).netloc
            name = domain.replace('www.', '').split('.')[0].title()
            return name[:200]
        except:
            return "Unknown Business"
    
    def _extract_description(self, soup):
        """Extract description"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc.get('content')[:500]
        
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 50 and len(text) < 300:
                return text[:500]
        
        return ""
    
    def _extract_phones(self, soup):
        """Extract phone numbers"""
        phones = set()
        text = soup.get_text()
        
        phone_patterns = [
            r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',
            r'\d{3}[-.\s]?\d{3}[-.\s]?\d{4}',
            r'\+1[-.\s]?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',
        ]
        
        for pattern in phone_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                phone = re.sub(r'[^\d+]', '', match)
                if len(phone) >= 10:
                    phones.add(phone)
        
        # Look in tel: links
        tel_links = soup.find_all('a', href=re.compile(r'tel:'))
        for link in tel_links:
            href = link.get('href', '')
            phone_match = re.search(r'tel:([\+\d\s\-\(\)]+)', href)
            if phone_match:
                phone = re.sub(r'[^\d+]', '', phone_match.group(1))
                if len(phone) >= 10:
                    phones.add(phone)
        
        return list(phones)[:3]
    
    def _extract_emails(self, soup):
        """Extract email addresses"""
        emails = set()
        text = soup.get_text()
        
        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        matches = re.findall(email_pattern, text)
        emails.update(matches)
        
        # Mailto links
        mailto_links = soup.find_all('a', href=re.compile(r'mailto:'))
        for link in mailto_links:
            href = link.get('href', '')
            email_match = re.search(r'mailto:([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', href)
            if email_match:
                emails.add(email_match.group(1))
        
        return list(emails)[:5]
    
    def _extract_address(self, soup):
        """Extract address"""
        text = soup.get_text()
        
        address_patterns = [
            r'\d+\s+[A-Za-z\s]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Drive|Dr|Lane|Ln|Way|Court|Ct|Place|Pl),?\s+[A-Za-z\s]+,\s+[A-Z]{2}\s+\d{5}',
            r'\d+\s+[A-Za-z\s]+,\s+[A-Za-z\s]+,\s+[A-Z]{2}\s+\d{5}',
        ]
        
        for pattern in address_patterns:
            matches = re.findall(pattern, text)
            if matches:
                return matches[0]
        
        # Look for address elements
        address_elements = soup.find_all(['address', 'div', 'span'], class_=re.compile(r'address|location|contact'))
        for elem in address_elements:
            text = elem.get_text(strip=True)
            if len(text) > 20 and len(text) < 200:
                if any(word in text.lower() for word in ['street', 'ave', 'road', 'rd', 'drive', 'dr']):
                    return text
        
        return ""
    
    def _extract_social_media(self, soup):
        """Extract social media links"""
        social_media = {}
        
        social_platforms = {
            'facebook': ['facebook.com', 'fb.com'],
            'instagram': ['instagram.com'],
            'linkedin': ['linkedin.com'],
            'twitter': ['twitter.com', 'x.com'],
            'youtube': ['youtube.com'],
            'tiktok': ['tiktok.com']
        }
        
        for platform, domains in social_platforms.items():
            for a in soup.find_all('a', href=True):
                href = a['href'].lower()
                for domain in domains:
                    if domain in href:
                        social_media[platform] = a['href']
                        break
        
        return social_media
    
    def _extract_services(self, soup):
        """Extract services"""
        services = []
        
        service_keywords = [
            'installation', 'repair', 'maintenance', 'service', 'contractor',
            'construction', 'remodeling', 'renovation', 'building', 'design',
            'cleaning', 'painting', 'electrical', 'plumbing', 'hvac', 'roofing',
            'landscaping', 'hardscaping', 'concrete', 'excavation', 'deck', 'fence'
        ]
        
        text_content = soup.get_text().lower()
        
        for keyword in service_keywords:
            if keyword in text_content:
                services.append(keyword.title())
        
        # Look for service sections
        for heading in soup.find_all(['h2', 'h3', 'h4']):
            heading_text = heading.get_text().lower()
            if any(word in heading_text for word in ['service', 'what we do', 'our work', 'expertise']):
                next_elem = heading.find_next()
                for _ in range(10):
                    if next_elem:
                        if next_elem.name in ['ul', 'ol']:
                            for li in next_elem.find_all('li'):
                                services.append(li.get_text(strip=True)[:100])
                        elif next_elem.name == 'p':
                            text = next_elem.get_text(strip=True)
                            if len(text) < 200:
                                services.append(text[:100])
                        next_elem = next_elem.find_next_sibling()
                    else:
                        break
        
        return list(set(services))[:10]
    
    def _extract_google_business(self, soup, business_name, city):
        """Extract Google Business Profile link"""
        for a in soup.find_all('a', href=True):
            href = a['href'].lower()
            if any(pattern in href for pattern in ['google.com/maps', 'g.page', 'goo.gl/maps', 'maps.app.goo.gl']):
                return a['href']
        
        for iframe in soup.find_all('iframe', src=True):
            src = iframe['src'].lower()
            if 'google.com/maps' in src:
                return src
        
        # Construct search URL
        if business_name and city:
            search_query = f"{business_name} {city} Google Business"
            encoded_query = quote(search_query)
            return f"https://www.google.com/search?q={encoded_query}"
        
        return ""
    
    def _check_google_ads(self, url):
        """Check if business is running Google Ads"""
        try:
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
            
            if not domain:
                return {"running_ads": False, "ad_transparency_url": ""}
            
            if domain.startswith('www.'):
                domain = domain[4:]
            
            ad_transparency_url = f"https://adstransparency.google.com/?region=US&domain={domain}"
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
            }
            
            try:
                response = requests.get(ad_transparency_url, headers=headers, timeout=8)
                soup = BeautifulSoup(response.text, 'html.parser')
                text = soup.get_text().lower()
                
                ad_indicators = [
                    'advertiser', 'campaign', 'political ads',
                    'election ads', 'verified advertiser'
                ]
                
                running_ads = any(indicator in text for indicator in ad_indicators)
                
                return {
                    "running_ads": running_ads,
                    "ad_transparency_url": ad_transparency_url
                }
                
            except:
                return {
                    "running_ads": False,
                    "ad_transparency_url": ad_transparency_url
                }
                
        except Exception as e:
            logger.log(f"Google Ads check error: {e}", "WARNING")
            return {"running_ads": False, "ad_transparency_url": ""}
    
    def _find_yelp_page(self, business_name, city):
        """Find Yelp page"""
        if not business_name or not city:
            return ""
        
        try:
            search_query = f"{business_name} {city} Yelp"
            encoded_query = quote(search_query)
            return f"https://www.yelp.com/search?find_desc={encoded_query}"
        except:
            return ""
    
    def _find_bbb_page(self, business_name, city):
        """Find BBB page"""
        if not business_name or not city:
            return ""
        
        try:
            search_query = f"{business_name} {city} BBB"
            encoded_query = quote(search_query)
            return f"https://www.bbb.org/search?find_desc={encoded_query}"
        except:
            return ""

# ============================================================================
# ENHANCED LEAD SCRAPER - 100% WORKING
# ============================================================================

class EnhancedLeadScraper:
    """Main scraper with website and directory support"""
    
    def __init__(self):
        self.api_key = CONFIG.get("serper_api_key", "")
        self.scraper = EnhancedWebsiteScraper()
        self.crm = CRM_Database()
        self.running = False
        self.paused = False
        self.stats = {
            'total_leads': 0,
            'qualified_leads': 0,
            'premium_leads': 0,
            'leads_without_website': 0,
            'leads_with_ads': 0,
            'directory_leads': 0,
            'cycles': 0,
            'last_cycle': None
        }
        self.cache_file = CONFIG["storage"]["cache_file"]
        self.load_cache()
    
    def load_cache(self):
        """Load search cache"""
        self.cache = {}
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r') as f:
                    self.cache = json.load(f)
            except:
                self.cache = {}
    
    def save_cache(self):
        """Save search cache"""
        try:
            with open(self.cache_file, 'w') as f:
                json.dump(self.cache, f, indent=2)
        except:
            pass
    
    def generate_search_queries(self):
        """Generate search queries - with directory support"""
        queries = []
        state = CONFIG["state"]
        
        # WEBSITE searches
        for industry in CONFIG["industries"]:
            for city in CONFIG["cities"]:
                for phrase_template in CONFIG["search_phrases"]:
                    query = phrase_template.format(
                        industry=industry,
                        city=city,
                        state=state
                    )
                    queries.append({
                        'query': query,
                        'industry': industry,
                        'city': city,
                        'state': state,
                        'type': 'website'
                    })
        
        # DIRECTORY searches (if enabled)
        if CONFIG["filters"]["include_directory_listings"]:
            for directory in CONFIG["directory_sources"]:
                for industry in CONFIG["industries"]:
                    for city in CONFIG["cities"]:
                        queries.append({
                            'query': f"{industry} {city} {state} {directory}",
                            'industry': industry,
                            'city': city,
                            'state': state,
                            'type': 'directory',
                            'directory': directory
                        })
        
        random.shuffle(queries)
        return queries[:CONFIG["searches_per_cycle"] * 2]
    
    def search_serper(self, query):
        """Search using Serper API"""
        if not self.api_key or self.api_key == "YOUR_SERPER_API_KEY":
            logger.log("No Serper API key configured", "ERROR")
            return []
        
        # Check cache
        cache_key = hashlib.md5(query.encode()).hexdigest()
        if cache_key in self.cache:
            logger.log(f"Using cached results for: {query}", "DEBUG")
            return self.cache[cache_key]
        
        try:
            url = "https://google.serper.dev/search"
            headers = {
                'X-API-KEY': self.api_key,
                'Content-Type': 'application/json'
            }
            payload = {
                'q': query,
                'num': CONFIG["businesses_per_search"]
            }
            
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            data = response.json()
            
            results = []
            
            if 'organic' in data:
                for item in data['organic']:
                    result = {
                        'title': item.get('title', ''),
                        'link': item.get('link', ''),
                        'snippet': item.get('snippet', ''),
                        'position': item.get('position', 0)
                    }
                    results.append(result)
            
            # Cache results
            self.cache[cache_key] = results
            self.save_cache()
            
            logger.log(f"Found {len(results)} results for: {query}", "INFO")
            return results
            
        except Exception as e:
            logger.log(f"Serper API error for '{query}': {e}", "ERROR")
            return []
    
    def is_blacklisted(self, url):
        """Check if domain is blacklisted"""
        if not url:
            return False
        
        domain = urlparse(url).netloc.lower()
        
        for blacklisted in CONFIG["blacklisted_domains"]:
            if blacklisted in domain:
                return True
        
        return False
    
    def generate_fingerprint(self, business_name, website, phone, city, directory_source=""):
        """Generate fingerprint for duplicate detection"""
        if directory_source:
            data = f"{business_name}_{directory_source}_{phone}_{city}"
        else:
            data = f"{business_name}_{website}_{phone}_{city}"
        return hashlib.md5(data.encode()).hexdigest()
    
    def qualify_lead(self, lead_data):
        """Qualify lead using AI"""
        if not CONFIG["ai_enrichment"]["enabled"] or not self.scraper.openai_client:
            # Basic scoring without AI
            lead_score = 50
            
            # Website factors
            if lead_data.get('website') and lead_data.get('has_website', True):
                lead_score += 20
            elif lead_data.get('phone'):
                lead_score += 15
            elif lead_data.get('address'):
                lead_score += 10
            
            if lead_data.get('email'):
                lead_score += 10
            
            if lead_data.get('running_google_ads'):
                lead_score += 25
            
            # Directory listing adjustment
            if lead_data.get('is_directory_listing', False):
                lead_score -= 10  # Slight penalty for directory-only listings
                if lead_data.get('rating', 0) >= 4.0:
                    lead_score += 15
                if lead_data.get('review_count', 0) >= 10:
                    lead_score += 10
            
            # Adjust based on completeness
            fields_present = 0
            if lead_data.get('business_name'): fields_present += 1
            if lead_data.get('phone'): fields_present += 1
            if lead_data.get('email'): fields_present += 1
            if lead_data.get('address'): fields_present += 1
            if lead_data.get('city'): fields_present += 1
            
            lead_score += (fields_present * 5)
            
            # Cap at 100
            lead_score = min(100, max(0, lead_score))
            
            if lead_score >= 80:
                quality_tier = "Premium"
            elif lead_score >= 60:
                quality_tier = "High"
            elif lead_score >= 40:
                quality_tier = "Medium"
            else:
                quality_tier = "Low"
            
            lead_data['lead_score'] = lead_score
            lead_data['quality_tier'] = quality_tier
            lead_data['business_type'] = "Unknown"
            lead_data['ai_notes'] = "Basic scoring applied"
            
            return lead_data
        
        try:
            # AI qualification
            prompt = f"""
            Analyze this business lead and provide:
            1. Lead score (0-100)
            2. Quality tier (Premium, High, Medium, Low, Unknown)
            3. Business type (LLC, Corporation, Sole Proprietorship, Partnership, Unknown)
            4. Key services
            5. AI notes with insights
            
            Lead Information:
            - Business: {lead_data.get('business_name', 'Unknown')}
            - Website: {lead_data.get('website', 'None')}
            - Has Website: {lead_data.get('has_website', True)}
            - Is Directory Listing: {lead_data.get('is_directory_listing', False)}
            - Directory Source: {lead_data.get('directory_source', 'None')}
            - Phone: {lead_data.get('phone', 'None')}
            - Email: {lead_data.get('email', 'None')}
            - City: {lead_data.get('city', 'Unknown')}
            - Industry: {lead_data.get('industry', 'Unknown')}
            - Running Google Ads: {lead_data.get('running_google_ads', False)}
            - Rating: {lead_data.get('rating', 'Not available')}
            - Review Count: {lead_data.get('review_count', 'Not available')}
            
            Scoring Criteria:
            - Website presence: +20 points
            - Contact info completeness: +5 per field (phone, email, address)
            - Google Ads running: +25 points
            - High ratings (â‰¥4.0): +15 points
            - Multiple reviews (â‰¥10): +10 points
            - Directory-only listing: -10 points
            
            Respond in JSON format:
            {{
                "lead_score": 0-100,
                "quality_tier": "Premium/High/Medium/Low/Unknown",
                "business_type": "LLC/Corporation/Sole Proprietorship/Partnership/Unknown",
                "services": ["service1", "service2"],
                "ai_notes": "Your analysis here"
            }}
            """
            
            response = self.scraper.openai_client.chat.completions.create(
                model=CONFIG["ai_enrichment"]["model"],
                messages=[
                    {"role": "system", "content": "You are a lead qualification expert for home services businesses."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=CONFIG["ai_enrichment"]["max_tokens"],
                temperature=0.3
            )
            
            ai_response = response.choices[0].message.content
            
            try:
                ai_data = json.loads(ai_response)
                
                lead_data['lead_score'] = ai_data.get('lead_score', 50)
                lead_data['quality_tier'] = ai_data.get('quality_tier', 'Unknown')
                lead_data['business_type'] = ai_data.get('business_type', 'Unknown')
                
                existing_services = lead_data.get('services', [])
                ai_services = ai_data.get('services', [])
                if isinstance(existing_services, str):
                    existing_services = [existing_services]
                if isinstance(ai_services, list):
                    all_services = list(set(existing_services + ai_services))
                    lead_data['services'] = all_services[:10]
                
                lead_data['ai_notes'] = ai_data.get('ai_notes', '')
                
            except:
                lead_data['lead_score'] = 50
                lead_data['quality_tier'] = 'Unknown'
                lead_data['ai_notes'] = 'AI analysis failed'
        
        except Exception as e:
            logger.log(f"AI qualification error: {e}", "WARNING")
            lead_data['lead_score'] = 50
            lead_data['quality_tier'] = 'Unknown'
            lead_data['ai_notes'] = f'AI error: {str(e)}'
        
        return lead_data
    
    def process_lead(self, search_result, meta_info):
        """Process a single search result"""
        url = search_result.get('link', '')
        title = search_result.get('title', '')
        snippet = search_result.get('snippet', '')
        
        # Skip blacklisted websites (but allow directory sites)
        if url and meta_info.get('type') == 'website' and self.is_blacklisted(url):
            return None
        
        # Extract business name
        business_name = title
        if not business_name or len(business_name) < 3:
            if snippet:
                name_match = re.search(r'^([A-Z][a-zA-Z\s&]+(?:Company|Services|Contractors|Contractor|LLC|Inc|Corp))', snippet)
                if name_match:
                    business_name = name_match.group(1)
        
        # Determine if this is a directory or website
        is_directory = False
        directory_source = ""
        
        if meta_info.get('type') == 'directory':
            is_directory = True
            directory_source = meta_info.get('directory', '')
        elif url:
            domain = urlparse(url).netloc.lower()
            for directory in CONFIG["directory_sources"]:
                if directory in domain:
                    is_directory = True
                    directory_source = directory
                    break
        
        # Scrape based on type
        if is_directory:
            # Directory listing
            scraped_data = self.scraper.scrape_directory(
                directory_source,
                business_name,
                meta_info.get('city', ''),
                meta_info.get('state', CONFIG['state'])
            )
        elif url and url.startswith(('http://', 'https://')):
            # Regular website
            scraped_data = self.scraper.scrape_website(url, business_name, meta_info.get('city', ''))
        else:
            # No URL, create basic directory listing
            scraped_data = {
                'website': '',
                'business_name': business_name if business_name else 'Unknown Business',
                'description': snippet[:200] if snippet else '',
                'phones': [],
                'emails': [],
                'address': '',
                'social_media': {},
                'services': [],
                'has_website': False,
                'is_directory_listing': True,
                'directory_source': 'search_result',
                'google_business_profile': '',
                'running_google_ads': False,
                'ad_transparency_url': '',
                'yelp_url': '',
                'bbb_url': '',
                'rating': 0,
                'review_count': 0
            }
            
            # Extract phone from snippet
            if snippet:
                phone_pattern = r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}'
                phones = re.findall(phone_pattern, snippet)
                if phones:
                    scraped_data['phones'] = [re.sub(r'[^\d+]', '', phones[0])]
            
            # Extract address from snippet
            address_pattern = r'\d+\s+[A-Za-z\s]+(?:Street|St|Avenue|Ave|Road|Rd)[^,]*,\s*[A-Za-z\s]+,\s*[A-Z]{2}'
            address_match = re.search(address_pattern, snippet)
            if address_match:
                scraped_data['address'] = address_match.group(0)
        
        # Create lead object
        lead_data = {
            'business_name': scraped_data.get('business_name', business_name or 'Unknown Business'),
            'website': scraped_data.get('website', url),
            'phone': scraped_data.get('phones', [''])[0] if scraped_data.get('phones') else '',
            'email': scraped_data.get('emails', [''])[0] if scraped_data.get('emails') else '',
            'address': scraped_data.get('address', ''),
            'city': meta_info.get('city', ''),
            'state': meta_info.get('state', CONFIG['state']),
            'industry': meta_info.get('industry', ''),
            'description': scraped_data.get('description', snippet[:500])[:500],
            'social_media': scraped_data.get('social_media', {}),
            'services': scraped_data.get('services', []),
            'has_website': scraped_data.get('has_website', True),
            'is_directory_listing': scraped_data.get('is_directory_listing', False),
            'directory_source': scraped_data.get('directory_source', ''),
            'google_business_profile': scraped_data.get('google_business_profile', ''),
            'running_google_ads': scraped_data.get('running_google_ads', False),
            'ad_transparency_url': scraped_data.get('ad_transparency_url', ''),
            'yelp_url': scraped_data.get('yelp_url', ''),
            'bbb_url': scraped_data.get('bbb_url', ''),
            'rating': scraped_data.get('rating', 0),
            'review_count': scraped_data.get('review_count', 0),
            'scraped_date': datetime.now(timezone.utc).isoformat()
        }
        
        # Generate fingerprint
        fingerprint = self.generate_fingerprint(
            lead_data['business_name'],
            lead_data['website'],
            lead_data['phone'],
            lead_data['city'],
            lead_data['directory_source']
        )
        lead_data['fingerprint'] = fingerprint
        
        # Apply filters
        if not self.passes_filters(lead_data):
            return None
        
        # AI qualification
        lead_data = self.qualify_lead(lead_data)
        
        return lead_data
    
    def passes_filters(self, lead_data):
        """Check if lead passes filters"""
        filters = CONFIG["filters"]
        
        # Exclude without website if setting is True
        if filters["exclude_without_websites"] and not lead_data.get('has_website', True):
            return False
        
        # Exclude without phone
        if filters["exclude_without_phone"] and not lead_data.get('phone'):
            return False
        
        # Check for chain keywords
        business_name = lead_data.get('business_name', '').lower()
        description = lead_data.get('description', '').lower()
        
        for keyword in filters["exclude_keywords"]:
            if keyword.lower() in business_name or keyword.lower() in description:
                return False
        
        # Directory-only filter
        if filters["directory_only_when_no_website"]:
            if lead_data.get('is_directory_listing', False) and lead_data.get('has_website', True):
                return False
        
        return True
    
    def run_cycle(self):
        """Run one scraping cycle"""
        if not self.running:
            return
        
        logger.log(f"ðŸš€ Starting scraping cycle {self.stats['cycles'] + 1}", "INFO")
        
        queries = self.generate_search_queries()
        leads_found = 0
        leads_without_website = 0
        leads_with_ads = 0
        directory_leads = 0
        
        for query_info in queries:
            if self.paused or not self.running:
                break
            
            query = query_info['query']
            logger.log(f"ðŸ” Searching: {query}", "INFO")
            
            results = self.search_serper(query)
            
            for result in results:
                if self.paused or not self.running:
                    break
                
                lead_data = self.process_lead(result, query_info)
                if lead_data:
                    # Update stats
                    if not lead_data.get('has_website', True):
                        leads_without_website += 1
                    if lead_data.get('running_google_ads', False):
                        leads_with_ads += 1
                    if lead_data.get('is_directory_listing', False):
                        directory_leads += 1
                    
                    # Save to CRM
                    if CONFIG["crm"]["enabled"] and CONFIG["crm"]["auto_sync"]:
                        result = self.crm.save_lead(lead_data)
                        if result["success"]:
                            leads_found += 1
                            logger.log(f"âœ… Saved lead: {lead_data['business_name']}", "SUCCESS")
                    
                    # Also save to JSON file
                    self.save_lead_to_file(lead_data)
            
            time.sleep(random.uniform(1, 3))
        
        self.stats['cycles'] += 1
        self.stats['total_leads'] += leads_found
        self.stats['leads_without_website'] += leads_without_website
        self.stats['leads_with_ads'] += leads_with_ads
        self.stats['directory_leads'] += directory_leads
        self.stats['last_cycle'] = datetime.now().isoformat()
        
        logger.log(f"âœ… Cycle completed. Found {leads_found} new leads ({leads_without_website} without websites, {directory_leads} directory leads, {leads_with_ads} with ads)", "SUCCESS")
    
    def save_lead_to_file(self, lead_data):
        """Save lead to JSON file"""
        try:
            leads_file = CONFIG["storage"]["leads_file"]
            leads = []
            
            if os.path.exists(leads_file):
                with open(leads_file, 'r') as f:
                    leads = json.load(f)
            
            leads.append(lead_data)
            
            # Keep only last 1000 leads
            if len(leads) > 1000:
                leads = leads[-1000:]
            
            with open(leads_file, 'w') as f:
                json.dump(leads, f, indent=2)
            
            # Separate directory leads
            if lead_data.get('is_directory_listing', False):
                directory_file = CONFIG["storage"]["directory_leads"]
                directory_leads = []
                
                if os.path.exists(directory_file):
                    with open(directory_file, 'r') as f:
                        directory_leads = json.load(f)
                
                directory_leads.append(lead_data)
                
                if len(directory_leads) > 500:
                    directory_leads = directory_leads[-500:]
                
                with open(directory_file, 'w') as f:
                    json.dump(directory_leads, f, indent=2)
            
            # Qualified leads
            if lead_data.get('lead_score', 0) >= CONFIG["ai_enrichment"]["qualification_threshold"]:
                qualified_file = CONFIG["storage"]["qualified_leads"]
                qualified = []
                
                if os.path.exists(qualified_file):
                    with open(qualified_file, 'r') as f:
                        qualified = json.load(f)
                
                qualified.append(lead_data)
                
                if len(qualified) > 500:
                    qualified = qualified[-500:]
                
                with open(qualified_file, 'w') as f:
                    json.dump(qualified, f, indent=2)
                
                # Premium leads (score >= 80)
                if lead_data.get('lead_score', 0) >= 80:
                    premium_file = CONFIG["storage"]["premium_leads"]
                    premium = []
                    
                    if os.path.exists(premium_file):
                        with open(premium_file, 'r') as f:
                            premium = json.load(f)
                    
                    premium.append(lead_data)
                    
                    if len(premium) > 100:
                        premium = premium[-100:]
                    
                    with open(premium_file, 'w') as f:
                        json.dump(premium, f, indent=2)
        
        except Exception as e:
            logger.log(f"Error saving lead to file: {e}", "WARNING")

# ============================================================================
# STREAMLIT DASHBOARD - 100% WORKING
# ============================================================================

class StreamlitDashboard:
    """Production-ready dashboard - 100% WORKING"""
    
    def __init__(self):
        if not STREAMLIT_AVAILABLE:
            self.enabled = False
            logger.log("Streamlit not available", "WARNING")
            return
        
        try:
            self.crm = CRM_Database()
            self.scraper = None
            self.scraper_running = False
            self.scraper_thread = None
            self.enabled = True
            
            st.set_page_config(
                page_title="LeadScraper CRM | Production",
                page_icon="ðŸš€",
                layout="wide",
                initial_sidebar_state="expanded"
            )
            
            self.setup_custom_css()
            
            logger.log("âœ… Dashboard initialized", "SUCCESS")
        except Exception as e:
            self.enabled = False
            logger.log(f"Dashboard error: {e}", "ERROR")
    
    def setup_custom_css(self):
        """Setup MitzMedia styling"""
        st.markdown("""
        <style>
        :root {
            --primary: #111827;
            --primary-dark: #0f172a;
            --accent: #f59e0b;
            --accent-light: #fbbf24;
            --success: #10b981;
            --danger: #ef4444;
            --warning: #f59e0b;
            --info: #3b82f6;
            --card-bg: #1e293b;
            --border: #334155;
            --text-light: #f1f5f9;
            --text-muted: #94a3b8;
            --text-dark: #0f172a;
        }
        
        .stApp {
            background: linear-gradient(135deg, var(--primary-dark) 0%, var(--card-bg) 100%) !important;
            color: var(--text-light) !important;
        }
        
        .stButton > button {
            background: linear-gradient(135deg, var(--accent) 0%, var(--accent-light) 100%) !important;
            color: #111827 !important;
            border: none !important;
            border-radius: 8px !important;
            font-weight: 600 !important;
            padding: 0.75rem 1.5rem !important;
        }
        
        .stButton > button:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 25px rgba(245, 158, 11, 0.3);
        }
        
        .badge {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 9999px;
            font-size: 0.75rem;
            font-weight: 600;
        }
        
        .badge-premium { background: linear-gradient(135deg, #f59e0b, #d97706); color: white; }
        .badge-high { background: linear-gradient(135deg, #10b981, #059669); color: white; }
        .badge-medium { background: linear-gradient(135deg, #3b82f6, #2563eb); color: white; }
        .badge-low { background: linear-gradient(135deg, #6b7280, #4b5563); color: white; }
        .badge-no-website { background: linear-gradient(135deg, #ef4444, #dc2626); color: white; }
        .badge-ads { background: linear-gradient(135deg, #8b5cf6, #7c3aed); color: white; }
        .badge-directory { background: linear-gradient(135deg, #ec4899, #db2777); color: white; }
        
        h1, h2, h3 {
            color: var(--text-light) !important;
        }
        
        h1 {
            background: linear-gradient(135deg, var(--accent) 0%, var(--accent-light) 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 1rem !important;
        }
        
        .metric-card {
            background: linear-gradient(135deg, rgba(30, 41, 59, 0.7) 0%, rgba(15, 23, 42, 0.7) 100%);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1rem;
        }
        
        .dataframe {
            border: 1px solid var(--border) !important;
            border-radius: 8px !important;
        }
        
        .dataframe th {
            background: rgba(30, 41, 59, 0.8) !important;
            color: var(--text-light) !important;
            font-weight: 600 !important;
        }
        
        .dataframe td {
            border-color: var(--border) !important;
            color: var(--text-muted) !important;
        }
        
        .stTabs [data-baseweb="tab-list"] {
            gap: 1rem;
        }
        
        .stTabs [data-baseweb="tab"] {
            background: rgba(30, 41, 59, 0.5);
            border-radius: 8px 8px 0 0;
            padding: 0.75rem 1.5rem;
            color: var(--text-muted);
        }
        
        .stTabs [aria-selected="true"] {
            background: rgba(245, 158, 11, 0.2) !important;
            color: var(--accent) !important;
        }
        
        /* Hide Streamlit branding */
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
        header {visibility: hidden;}
        </style>
        """, unsafe_allow_html=True)
    
    def run_scraper_background(self):
        """Run scraper in background"""
        try:
            self.scraper = EnhancedLeadScraper()
            self.scraper.running = True
            
            cycles = 0
            while self.scraper_running and cycles < CONFIG['max_cycles']:
                if not self.scraper.running:
                    break
                
                self.scraper.run_cycle()
                cycles += 1
                
                if 'scraper_stats' not in st.session_state:
                    st.session_state.scraper_stats = {}
                
                st.session_state.scraper_stats = {
                    'cycles': cycles,
                    'total_leads': self.scraper.stats['total_leads'],
                    'leads_without_website': self.scraper.stats['leads_without_website'],
                    'leads_with_ads': self.scraper.stats['leads_with_ads'],
                    'directory_leads': self.scraper.stats['directory_leads'],
                    'last_cycle': self.scraper.stats['last_cycle']
                }
                
                if self.scraper_running and cycles < CONFIG['max_cycles']:
                    time.sleep(CONFIG['cycle_interval'])
            
            self.scraper_running = False
            st.session_state.scraper_running = False
            
        except Exception as e:
            logger.log(f"Background scraper error: {e}", "ERROR")
            self.scraper_running = False
            st.session_state.scraper_running = False
    
    def start_scraper(self):
        """Start scraper"""
        if not self.scraper_running:
            self.scraper_running = True
            st.session_state.scraper_running = True
            self.scraper_thread = threading.Thread(target=self.run_scraper_background, daemon=True)
            self.scraper_thread.start()
            return True
        return False
    
    def stop_scraper(self):
        """Stop scraper"""
        self.scraper_running = False
        if self.scraper:
            self.scraper.running = False
        st.session_state.scraper_running = False
        return True
    
    def render_sidebar(self):
        """Render sidebar"""
        with st.sidebar:
            st.markdown("""
            <div style="text-align: center; margin-bottom: 2rem;">
                <div style="font-size: 2.5rem;">ðŸš€</div>
                <h1 style="color: #f59e0b; margin: 0;">Mitz</h1>
                <p style="color: #94a3b8; margin: 0;">automated crm</p>
            </div>
            """, unsafe_allow_html=True)
            
            # Navigation
            page = st.radio(
                "ðŸ“Š Navigation",
                ["Dashboard", "Leads", "Lead Details", "Settings", "Logs", "Export"],
                label_visibility="collapsed"
            )
            
            st.divider()
            
            # Scraper Control
            st.markdown("### âš™ï¸ Scraper Control")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("â–¶ï¸ Start", use_container_width=True, type="primary"):
                    if self.start_scraper():
                        st.success("Scraper started!")
                        st.rerun()
            
            with col2:
                if st.button("â¹ï¸ Stop", use_container_width=True, type="secondary"):
                    if self.stop_scraper():
                        st.info("Scraper stopped!")
                        st.rerun()
            
            # Status
            if 'scraper_running' not in st.session_state:
                st.session_state.scraper_running = False
            
            status_color = "#10b981" if st.session_state.scraper_running else "#ef4444"
            status_text = "Active" if st.session_state.scraper_running else "Stopped"
            
            st.markdown(f"""
            <div class="metric-card">
                <div style="display: flex; justify-content: space-between; align-items: center;">
                    <span style="color: {status_color}; font-weight: 600;">{status_text}</span>
                    <span style="color: #94a3b8;">Status</span>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            if 'scraper_stats' in st.session_state:
                stats = st.session_state.scraper_stats
                st.markdown(f"""
                <div class="metric-card">
                    <div style="display: flex; justify-content: space-between; margin-bottom: 0.5rem;">
                        <span style="color: #94a3b8;">Cycles</span>
                        <span style="color: #f59e0b; font-weight: 600;">{stats.get('cycles', 0)}</span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-bottom: 0.5rem;">
                        <span style="color: #94a3b8;">Total Leads</span>
                        <span style="color: #10b981; font-weight: 600;">{stats.get('total_leads', 0)}</span>
                    </div>
                    <div style="display: flex; justify-content: space-between;">
                        <span style="color: #94a3b8;">Directory Leads</span>
                        <span style="color: #ec4899; font-weight: 600;">{stats.get('directory_leads', 0)}</span>
                    </div>
                </div>
                """, unsafe_allow_html=True)
            
            st.divider()
            
            # Quick Stats
            st.markdown("### ðŸ“ˆ Quick Stats")
            
            today_count = self.crm.get_today_count()
            total_leads = self.crm.get_leads()["total"]
            stats = self.crm.get_statistics()
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Today", today_count)
            with col2:
                st.metric("Total", total_leads)
            
            col3, col4 = st.columns(2)
            with col3:
                without_website = stats["overall"].get("leads_without_website", 0)
                st.metric("No Website", without_website)
            with col4:
                directory_leads = stats["overall"].get("directory_leads", 0)
                st.metric("Directory", directory_leads)
            
            st.divider()
            
            # System Info
            st.markdown("### ðŸ’» System Info")
            
            enhanced_features = CONFIG.get('enhanced_features', {})
            filters = CONFIG.get('filters', {})
            
            info_items = [
                ("Database", "âœ… Connected"),
                ("No Website Scraping", "âœ… Enabled" if not filters.get('exclude_without_websites', True) else "âŒ Disabled"),
                ("Directory Listings", "âœ… Enabled" if filters.get('include_directory_listings', True) else "âŒ Disabled"),
                ("Google Ads Check", "âœ… Enabled" if enhanced_features.get('check_google_ads', True) else "âŒ Disabled"),
                ("Google Business", "âœ… Enabled" if enhanced_features.get('find_google_business', True) else "âŒ Disabled"),
                ("AI Enrichment", "âœ… Enabled" if CONFIG.get('ai_enrichment', {}).get('enabled', True) else "âŒ Disabled")
            ]
            
            for label, value in info_items:
                st.markdown(f"**{label}:** {value}")
            
            st.markdown(f"**State:** {CONFIG.get('state', 'PA')}")
            st.markdown(f"**Cities:** {len(CONFIG.get('cities', []))}")
            st.markdown(f"**Industries:** {len(CONFIG.get('industries', []))}")
            
            return page
    
    def render_dashboard(self):
        """Render dashboard page - 100% WORKING"""
        st.title("ðŸ“Š Dashboard Overview")
        
        stats = self.crm.get_statistics()
        
        # Top Metrics Row
        col1, col2, col3, col4, col5, col6 = st.columns(6)
        
        with col1:
            st.metric("Total Leads", stats["overall"]["total_leads"])
        with col2:
            st.metric("Estimated Value", f"${stats['overall']['total_value']:,}")
        with col3:
            st.metric("Avg Score", f"{stats['overall']['avg_score']:.1f}")
        with col4:
            without_website = stats["overall"].get("leads_without_website", 0)
            st.metric("No Website", without_website)
        with col5:
            directory_leads = stats["overall"].get("directory_leads", 0)
            st.metric("Directory Leads", directory_leads)
        with col6:
            with_ads = stats["overall"].get("leads_with_ads", 0)
            st.metric("With Ads", with_ads)
        
        # Charts Row 1
        col1, col2 = st.columns(2)
        
        with col1:
            # Quality distribution
            quality_data = stats["quality_distribution"]
            if quality_data:
                df_quality = pd.DataFrame(quality_data)
                fig_quality = px.pie(
                    df_quality, 
                    values='count', 
                    names='tier',
                    title='Lead Quality Distribution',
                    color='tier',
                    color_discrete_map={
                        'Premium': '#f59e0b',
                        'High': '#10b981',
                        'Medium': '#3b82f6',
                        'Low': '#6b7280'
                    }
                )
                fig_quality.update_layout(
                    plot_bgcolor='rgba(0,0,0,0)',
                    paper_bgcolor='rgba(0,0,0,0)',
                    font_color='#f1f5f9',
                    showlegend=True,
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    )
                )
                st.plotly_chart(fig_quality, use_container_width=True)
            else:
                st.info("No quality distribution data available")
        
        with col2:
            # Source distribution (Website vs Directory)
            source_data = stats.get("source_distribution", [])
            if source_data:
                df_source = pd.DataFrame(source_data)
                fig_source = px.bar(
                    df_source,
                    x='source',
                    y='count',
                    title='Lead Source Distribution',
                    color='source',
                    color_discrete_map={
                        'Website': '#3b82f6',
                        'Directory': '#ec4899'
                    }
                )
                fig_source.update_layout(
                    plot_bgcolor='rgba(0,0,0,0)',
                    paper_bgcolor='rgba(0,0,0,0)',
                    font_color='#f1f5f9',
                    showlegend=False
                )
                st.plotly_chart(fig_source, use_container_width=True)
            else:
                st.info("No source distribution data available")
        
        # Charts Row 2
        col1, col2 = st.columns(2)
        
        with col1:
            # Website distribution
            website_data = stats.get("website_distribution", [])
            if website_data:
                df_website = pd.DataFrame(website_data)
                fig_website = px.bar(
                    df_website,
                    x='category',
                    y='count',
                    title='Website Status',
                    color='category',
                    color_discrete_map={
                        'Has Website': '#10b981',
                        'No Website': '#ef4444'
                    }
                )
                fig_website.update_layout(
                    plot_bgcolor='rgba(0,0,0,0)',
                    paper_bgcolor='rgba(0,0,0,0)',
                    font_color='#f1f5f9',
                    showlegend=False
                )
                st.plotly_chart(fig_website, use_container_width=True)
            else:
                st.info("No website distribution data available")
        
        with col2:
            # Status distribution
            status_data = stats.get("status_distribution", [])
            if status_data:
                df_status = pd.DataFrame(status_data)
                fig_status = px.bar(
                    df_status,
                    x='status',
                    y='count',
                    title='Lead Status Distribution',
                    color='count',
                    color_continuous_scale='blues'
                )
                fig_status.update_layout(
                    plot_bgcolor='rgba(0,0,0,0)',
                    paper_bgcolor='rgba(0,0,0,0)',
                    font_color='#f1f5f9',
                    showlegend=False,
                    xaxis_tickangle=-45
                )
                st.plotly_chart(fig_status, use_container_width=True)
            else:
                st.info("No status distribution data available")
        
        # Daily Leads Chart
        st.subheader("ðŸ“… Daily Lead Acquisition (Last 30 Days)")
        daily_data = stats["daily_leads"]
        if daily_data:
            df_daily = pd.DataFrame(daily_data)
            df_daily['date'] = pd.to_datetime(df_daily['date'])
            df_daily = df_daily.sort_values('date')
            
            fig_daily = px.line(
                df_daily,
                x='date',
                y='count',
                title='',
                markers=True,
                line_shape='spline'
            )
            fig_daily.update_layout(
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)',
                font_color='#f1f5f9',
                xaxis_title="Date",
                yaxis_title="Leads"
            )
            st.plotly_chart(fig_daily, use_container_width=True)
        else:
            st.info("No daily lead data available")
        
        # Recent Leads
        st.subheader("ðŸ†• Recent Leads")
        leads_data = self.crm.get_leads(page=1, per_page=10)
        
        if leads_data["leads"]:
            for lead in leads_data["leads"][:5]:
                with st.container():
                    col1, col2, col3 = st.columns([3, 1, 1])
                    with col1:
                        st.markdown(f"**{lead.get('business_name', 'Unknown')}**")
                        city = lead.get('city', '')
                        industry = lead.get('industry', '')
                        st.caption(f"{city} â€¢ {industry}" if city or industry else "Location unknown")
                    with col2:
                        score = lead.get('lead_score', 0)
                        tier = lead.get('quality_tier', 'Unknown')
                        tier_class = f"badge-{tier.lower()}" if tier.lower() in ['premium', 'high', 'medium', 'low'] else "badge-low"
                        st.markdown(f'<span class="{tier_class}">{tier}</span>', unsafe_allow_html=True)
                    with col3:
                        has_website = lead.get('has_website', True)
                        is_directory = lead.get('is_directory_listing', False)
                        if is_directory:
                            st.markdown('<span class="badge-directory">Directory</span>', unsafe_allow_html=True)
                        elif not has_website:
                            st.markdown('<span class="badge-no-website">No Website</span>', unsafe_allow_html=True)
                    st.divider()
        else:
            st.info("No leads found. Start the scraper to collect leads!")
    
    def render_leads(self):
        """Render leads management page"""
        st.title("ðŸ‘¥ Leads Management")
        
        # Advanced Filters
        with st.expander("ðŸ” Advanced Filters", expanded=True):
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                search_term = st.text_input("Search", placeholder="Business name, phone, email...")
            
            with col2:
                status_options = ["All"] + CONFIG["lead_management"]["status_options"]
                status_filter = st.selectbox("Status", status_options)
            
            with col3:
                quality_options = ["All"] + CONFIG["lead_management"]["quality_tiers"]
                quality_filter = st.selectbox("Quality", quality_options)
            
            with col4:
                city_options = ["All"] + CONFIG["cities"]
                city_filter = st.selectbox("City", city_options)
            
            # Additional filters
            col5, col6, col7, col8 = st.columns(4)
            
            with col5:
                website_filter = st.selectbox("Has Website", ["All", "Yes", "No"])
            
            with col6:
                directory_filter = st.selectbox("Directory Listing", ["All", "Yes", "No"])
            
            with col7:
                ads_filter = st.selectbox("Running Ads", ["All", "Yes", "No"])
            
            with col8:
                score_min, score_max = st.slider("Lead Score", 0, 100, (0, 100))
        
        # Build filters
        filters = {}
        if search_term:
            filters["search"] = search_term
        if status_filter != "All":
            filters["status"] = status_filter
        if quality_filter != "All":
            filters["quality_tier"] = quality_filter
        if city_filter != "All":
            filters["city"] = city_filter
        if website_filter == "Yes":
            filters["has_website"] = 1
        elif website_filter == "No":
            filters["has_website"] = 0
        if directory_filter == "Yes":
            filters["is_directory"] = 1
        elif directory_filter == "No":
            filters["is_directory"] = 0
        if ads_filter == "Yes":
            filters["running_ads"] = 1
        elif ads_filter == "No":
            filters["running_ads"] = 0
        
        filters["score_min"] = score_min
        filters["score_max"] = score_max
        
        # Get leads
        leads_data = self.crm.get_leads(filters=filters, page=1, per_page=50)
        
        st.metric("Total Leads Found", leads_data["total"])
        
        if leads_data["leads"]:
            # Create dataframe for display
            leads = leads_data["leads"]
            display_data = []
            
            for lead in leads[:50]:
                website_status = "âœ…" if lead.get("has_website") else "âŒ"
                ads_status = "âœ…" if lead.get("running_google_ads") else "âŒ"
                directory_status = "âœ…" if lead.get("is_directory_listing") else "âŒ"
                
                display_data.append({
                    "ID": lead.get("id"),
                    "Business": lead.get("business_name", "")[:30],
                    "Phone": lead.get("phone", ""),
                    "City": lead.get("city", ""),
                    "Score": lead.get("lead_score", 0),
                    "Quality": lead.get("quality_tier", "Unknown"),
                    "Status": lead.get("lead_status", "New Lead"),
                    "Website": website_status,
                    "Ads": ads_status,
                    "Directory": directory_status
                })
            
            if display_data:
                df = pd.DataFrame(display_data)
                st.dataframe(df, use_container_width=True, hide_index=True)
        else:
            st.info("No leads match the filters")
    
    def render_lead_details(self):
        """Render lead details page"""
        st.title("ðŸ“‹ Lead Details")
        
        # Lead selection
        col1, col2 = st.columns([3, 1])
        
        with col1:
            lead_id = st.number_input("Enter Lead ID", min_value=1, value=1, step=1)
        
        with col2:
            if st.button("Load Lead", type="primary"):
                st.session_state.selected_lead_id = lead_id
        
        if 'selected_lead_id' not in st.session_state:
            st.session_state.selected_lead_id = lead_id
        else:
            lead_id = st.session_state.selected_lead_id
        
        # Load lead
        lead = self.crm.get_lead_by_id(lead_id)
        
        if not lead:
            st.error(f"Lead with ID {lead_id} not found!")
            return
        
        # Create tabs for different sections
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["ðŸ“‹ Basic Info", "ðŸ“ž Contact", "ðŸ“Š Status & Actions", "ðŸ“ Activities", "ðŸŽ¯ AI Analysis"])
        
        with tab1:
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Business Information")
                
                st.text_input("Business Name", lead.get('business_name', ''), disabled=True)
                
                col_ind, col_type = st.columns(2)
                with col_ind:
                    st.text_input("Industry", lead.get('industry', ''), disabled=True)
                with col_type:
                    st.text_input("Business Type", lead.get('business_type', ''), disabled=True)
                
                # Quality Score
                col_score, col_tier = st.columns(2)
                with col_score:
                    st.metric("Lead Score", lead.get('lead_score', 0))
                with col_tier:
                    tier = lead.get('quality_tier', 'Unknown')
                    tier_color = {
                        'Premium': '#f59e0b',
                        'High': '#10b981',
                        'Medium': '#3b82f6',
                        'Low': '#6b7280'
                    }.get(tier, '#6b7280')
                    st.markdown(f"""
                    <div style="background-color: {tier_color}; color: white; padding: 0.5rem; border-radius: 6px; text-align: center;">
                        <strong>{tier}</strong>
                    </div>
                    """, unsafe_allow_html=True)
                
                # Source info
                source_info = []
                if lead.get('has_website'):
                    source_info.append("ðŸŒ Has Website")
                if lead.get('is_directory_listing'):
                    source_info.append("ðŸ“‹ Directory Listing")
                if lead.get('running_google_ads'):
                    source_info.append("ðŸŽ¯ Running Google Ads")
                
                if source_info:
                    st.markdown("**Source Info:**")
                    for info in source_info:
                        st.markdown(f"- {info}")
            
            with col2:
                st.subheader("Description & Services")
                st.text_area("Description", lead.get('description', ''), height=150, disabled=True)
                
                # Services
                services = lead.get('services', [])
                if isinstance(services, str):
                    services = [services]
                
                if services:
                    st.markdown("**Services:**")
                    cols = st.columns(2)
                    for i, service in enumerate(services[:10]):
                        col_idx = i % 2
                        with cols[col_idx]:
                            st.markdown(f"- {service}")
        
        with tab2:
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Contact Details")
                
                website = lead.get('website', '')
                if website:
                    st.markdown(f"**Website:** [{website}]({website})")
                else:
                    st.markdown("**Website:** Not available")
                
                phone = lead.get('phone', '')
                if phone:
                    st.markdown(f"**Phone:** `{phone}`")
                else:
                    st.markdown("**Phone:** Not available")
                
                email = lead.get('email', '')
                if email:
                    st.markdown(f"**Email:** `{email}`")
                else:
                    st.markdown("**Email:** Not available")
                
                address = lead.get('address', '')
                if address:
                    st.text_area("Address", address, disabled=True)
            
            with col2:
                st.subheader("Location")
                
                col_city, col_state = st.columns(2)
                with col_city:
                    st.text_input("City", lead.get('city', ''), disabled=True)
                with col_state:
                    st.text_input("State", lead.get('state', ''), disabled=True)
                
                # Social Media
                social_media = lead.get('social_media', {})
                if social_media and isinstance(social_media, dict) and social_media:
                    st.subheader("Social Media")
                    for platform, url in social_media.items():
                        st.markdown(f"**{platform.title()}:** [{url}]({url})")
                
                # Directory Links
                yelp_url = lead.get('yelp_url', '')
                bbb_url = lead.get('bbb_url', '')
                
                if yelp_url or bbb_url:
                    st.subheader("Directory Links")
                    if yelp_url:
                        st.markdown(f"**Yelp:** [{yelp_url}]({yelp_url})")
                    if bbb_url:
                        st.markdown(f"**BBB:** [{bbb_url}]({bbb_url})")
        
        with tab3:
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Lead Status")
                
                # Status update form
                with st.form("update_status_form"):
                    current_status = lead.get('lead_status', 'New Lead')
                    status_options = CONFIG["lead_management"]["status_options"]
                    current_index = status_options.index(current_status) if current_status in status_options else 0
                    
                    new_status = st.selectbox(
                        "Update Status",
                        status_options,
                        index=current_index
                    )
                    
                    priority_options = CONFIG["lead_management"]["priority_options"]
                    current_priority = lead.get('outreach_priority', 'Medium')
                    priority_index = priority_options.index(current_priority) if current_priority in priority_options else 2
                    
                    new_priority = st.selectbox(
                        "Update Priority",
                        priority_options,
                        index=priority_index
                    )
                    
                    assigned_to = st.text_input("Assigned To", lead.get('assigned_to', ''))
                    
                    notes = st.text_area("Notes", lead.get('notes', ''), height=100)
                    
                    col_submit, col_archive = st.columns(2)
                    with col_submit:
                        update_submitted = st.form_submit_button("ðŸ’¾ Update Lead", type="primary")
                    with col_archive:
                        archive_submitted = st.form_submit_button("ðŸ—„ï¸ Archive Lead", type="secondary")
                    
                    if update_submitted:
                        update_data = {
                            'lead_status': new_status,
                            'outreach_priority': new_priority,
                            'assigned_to': assigned_to,
                            'notes': notes
                        }
                        result = self.crm.update_lead(lead_id, update_data)
                        if result['success']:
                            st.success("Lead updated successfully!")
                            st.rerun()
                        else:
                            st.error(f"Error: {result['message']}")
                    
                    if archive_submitted:
                        result = self.crm.delete_lead(lead_id)
                        if result['success']:
                            st.success("Lead archived successfully!")
                            st.rerun()
                        else:
                            st.error(f"Error: {result['message']}")
            
            with col2:
                st.subheader("Timeline")
                
                created = lead.get('created_at', '')
                if created:
                    st.text(f"Created: {created[:19]}")
                
                scraped = lead.get('scraped_date', '')
                if scraped:
                    st.text(f"Scraped: {scraped[:19]}")
                
                updated = lead.get('last_updated', '')
                if updated:
                    st.text(f"Last Updated: {updated[:19]}")
                
                follow_up = lead.get('follow_up_date', '')
                if follow_up:
                    st.text(f"Follow-up Date: {follow_up}")
                
                st.subheader("Financial Info")
                st.metric("Potential Value", f"${lead.get('potential_value', 0):,}")
        
        with tab4:
            st.subheader("Activity Timeline")
            activities = lead.get('activities', [])
            
            if activities:
                for activity in activities[:20]:
                    with st.container():
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"**{activity.get('activity_type', 'Activity')}**")
                            st.caption(activity.get('activity_details', ''))
                        with col2:
                            performed = activity.get('performed_at', '')
                            if performed:
                                st.caption(performed[:19])
                        st.divider()
            else:
                st.info("No activities recorded yet.")
        
        with tab5:
            st.subheader("AI Analysis")
            
            ai_notes = lead.get('ai_notes', '')
            if ai_notes:
                st.text_area("AI Notes", ai_notes, height=200, disabled=True)
            else:
                st.info("No AI analysis available for this lead.")
    
    def render_settings(self):
        """Render settings page - 100% WORKING"""
        st.title("âš™ï¸ Settings")
        
        # Create tabs for different setting categories
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
            "ðŸ”‘ API Keys", 
            "ðŸŽ¯ Targeting", 
            "ðŸ” Scraper", 
            "ðŸ¢ Business", 
            "ðŸ¤– AI", 
            "ðŸ“Š CRM"
        ])
        
        with tab1:
            st.subheader("API Configuration")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Serper API
                current_serper = CONFIG.get("serper_api_key", "")
                new_serper = st.text_input("Serper API Key", value=current_serper, type="password")
                if new_serper != current_serper:
                    CONFIG["serper_api_key"] = new_serper
            
            with col2:
                # OpenAI API
                current_openai = CONFIG.get("openai_api_key", "")
                new_openai = st.text_input("OpenAI API Key", value=current_openai, type="password")
                if new_openai != current_openai:
                    CONFIG["openai_api_key"] = new_openai
            
            if st.button("Save API Keys", type="primary"):
                try:
                    with open(CONFIG_FILE, "w") as f:
                        json.dump(CONFIG, f, indent=2)
                    st.success("API keys saved!")
                except Exception as e:
                    st.error(f"Error: {e}")
        
        with tab2:
            st.subheader("ðŸŽ¯ Targeting Settings")
            
            # Website targeting
            st.markdown("### ðŸŒ Website Targeting")
            col1, col2 = st.columns(2)
            
            with col1:
                # No website scraping
                exclude_websites = CONFIG["filters"].get("exclude_without_websites", False)
                new_setting = st.toggle(
                    "Include businesses WITHOUT websites",
                    value=not exclude_websites,
                    help="Turn ON to get leads from directory sites (Yelp, BBB, etc.) with only phone/address"
                )
                CONFIG["filters"]["exclude_without_websites"] = not new_setting
            
            with col2:
                # Directory listings
                include_directories = CONFIG["filters"].get("include_directory_listings", True)
                new_directories = st.toggle(
                    "Include directory listings",
                    value=include_directories,
                    help="Include leads from Yelp, YellowPages, BBB, etc."
                )
                CONFIG["filters"]["include_directory_listings"] = new_directories
            
            # Directory-only filter
            directory_only = CONFIG["filters"].get("directory_only_when_no_website", True)
            new_directory_only = st.toggle(
                "Directory listings only when NO website",
                value=directory_only,
                help="Only use directory sites for businesses that don't have their own website"
            )
            CONFIG["filters"]["directory_only_when_no_website"] = new_directory_only
            
            # Directory sources
            st.markdown("### ðŸ“‹ Directory Sources")
            directory_text = st.text_area(
                "Directory websites (one per line)",
                value="\n".join(CONFIG.get("directory_sources", [])),
                height=150,
                help="Websites to scrape for businesses without their own websites"
            )
            if directory_text:
                CONFIG["directory_sources"] = [d.strip() for d in directory_text.split("\n") if d.strip()]
            
            # Blacklisted domains
            st.markdown("### ðŸš« Blacklisted Domains")
            blacklist_text = st.text_area(
                "Blacklisted domains (one per line)",
                value="\n".join(CONFIG.get("blacklisted_domains", [])),
                height=150,
                help="Domains to exclude from scraping"
            )
            if blacklist_text:
                CONFIG["blacklisted_domains"] = [d.strip() for d in blacklist_text.split("\n") if d.strip()]
            
            # Filter keywords
            st.markdown("### ðŸ” Filter Keywords")
            keywords_text = st.text_area(
                "Exclude keywords (comma-separated)",
                value=", ".join(CONFIG["filters"].get("exclude_keywords", [])),
                height=100,
                help="Businesses containing these keywords will be excluded"
            )
            if keywords_text:
                CONFIG["filters"]["exclude_keywords"] = [k.strip() for k in keywords_text.split(",") if k.strip()]
            
            if st.button("Save Targeting Settings", type="primary"):
                try:
                    with open(CONFIG_FILE, "w") as f:
                        json.dump(CONFIG, f, indent=2)
                    st.success("Targeting settings saved!")
                except Exception as e:
                    st.error(f"Error: {e}")
        
        with tab3:
            st.subheader("ðŸ” Scraper Settings")
            
            col1, col2 = st.columns(2)
            
            with col1:
                CONFIG["state"] = st.text_input("State", value=CONFIG.get("state", "PA"))
                CONFIG["searches_per_cycle"] = st.number_input(
                    "Searches per Cycle",
                    value=CONFIG.get("searches_per_cycle", 5),
                    min_value=1, max_value=50
                )
                CONFIG["businesses_per_search"] = st.number_input(
                    "Businesses per Search",
                    value=CONFIG.get("businesses_per_search", 10),
                    min_value=1, max_value=100
                )
            
            with col2:
                CONFIG["cycle_interval"] = st.number_input(
                    "Cycle Interval (seconds)",
                    value=CONFIG.get("cycle_interval", 300),
                    min_value=10, max_value=3600
                )
                CONFIG["max_cycles"] = st.number_input(
                    "Max Cycles",
                    value=CONFIG.get("max_cycles", 100),
                    min_value=1, max_value=1000
                )
                CONFIG["operating_mode"] = st.selectbox(
                    "Operating Mode",
                    options=["auto", "manual"],
                    index=0 if CONFIG.get("operating_mode", "auto") == "auto" else 1
                )
            
            # Enhanced features
            st.markdown("### ðŸš€ Enhanced Features")
            
            enhanced = CONFIG.get("enhanced_features", {})
            col1, col2, col3 = st.columns(3)
            
            with col1:
                CONFIG["enhanced_features"]["check_google_ads"] = st.toggle(
                    "Check Google Ads",
                    value=enhanced.get("check_google_ads", True)
                )
                CONFIG["enhanced_features"]["find_google_business"] = st.toggle(
                    "Find Google Business",
                    value=enhanced.get("find_google_business", True)
                )
            
            with col2:
                CONFIG["enhanced_features"]["auto_social_media"] = st.toggle(
                    "Extract Social Media",
                    value=enhanced.get("auto_social_media", True)
                )
                CONFIG["enhanced_features"]["extract_services"] = st.toggle(
                    "Extract Services",
                    value=enhanced.get("extract_services", True)
                )
            
            with col3:
                CONFIG["enhanced_features"]["lead_scoring_ai"] = st.toggle(
                    "AI Lead Scoring",
                    value=enhanced.get("lead_scoring_ai", True)
                )
                CONFIG["enhanced_features"]["detect_chain_businesses"] = st.toggle(
                    "Detect Chain Businesses",
                    value=enhanced.get("detect_chain_businesses", True)
                )
            
            if st.button("Save Scraper Settings", type="primary"):
                try:
                    with open(CONFIG_FILE, "w") as f:
                        json.dump(CONFIG, f, indent=2)
                    st.success("Scraper settings saved!")
                except Exception as e:
                    st.error(f"Error: {e}")
        
        with tab4:
            st.subheader("ðŸ¢ Business Settings")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### ðŸ™ï¸ Cities")
                cities_text = st.text_area(
                    "Cities (one per line)",
                    value="\n".join(CONFIG.get("cities", [])),
                    height=200
                )
                if cities_text:
                    CONFIG["cities"] = [city.strip() for city in cities_text.split("\n") if city.strip()]
            
            with col2:
                st.markdown("### ðŸ­ Industries")
                industries_text = st.text_area(
                    "Industries (one per line)",
                    value="\n".join(CONFIG.get("industries", [])),
                    height=200
                )
                if industries_text:
                    CONFIG["industries"] = [industry.strip() for industry in industries_text.split("\n") if industry.strip()]
            
            st.markdown("### ðŸ” Search Phrases")
            search_phrases_text = st.text_area(
                "Search Phrases (one per line)",
                value="\n".join(CONFIG.get("search_phrases", [])),
                height=150,
                help="Use {industry}, {city}, {state} as placeholders"
            )
            if search_phrases_text:
                CONFIG["search_phrases"] = [phrase.strip() for phrase in search_phrases_text.split("\n") if phrase.strip()]
            
            if st.button("Save Business Settings", type="primary"):
                try:
                    with open(CONFIG_FILE, "w") as f:
                        json.dump(CONFIG, f, indent=2)
                    st.success("Business settings saved!")
                except Exception as e:
                    st.error(f"Error: {e}")
        
        with tab5:
            st.subheader("ðŸ¤– AI Enrichment")
            
            ai_config = CONFIG.get("ai_enrichment", {})
            
            CONFIG["ai_enrichment"]["enabled"] = st.toggle(
                "Enable AI Enrichment",
                value=ai_config.get("enabled", True)
            )
            
            if CONFIG["ai_enrichment"]["enabled"]:
                col1, col2 = st.columns(2)
                
                with col1:
                    CONFIG["ai_enrichment"]["model"] = st.selectbox(
                        "Model",
                        options=["gpt-4o-mini", "gpt-4", "gpt-3.5-turbo"],
                        index=0 if ai_config.get("model", "gpt-4o-mini") == "gpt-4o-mini" else 
                              1 if ai_config.get("model") == "gpt-4" else 2
                    )
                    CONFIG["ai_enrichment"]["max_tokens"] = st.number_input(
                        "Max Tokens",
                        value=ai_config.get("max_tokens", 2000),
                        min_value=100, max_value=4000
                    )
                
                with col2:
                    CONFIG["ai_enrichment"]["qualification_threshold"] = st.slider(
                        "Qualification Threshold",
                        min_value=0, max_value=100,
                        value=ai_config.get("qualification_threshold", 60)
                    )
                    CONFIG["ai_enrichment"]["auto_qualify"] = st.toggle(
                        "Auto Qualify",
                        value=ai_config.get("auto_qualify", True)
                    )
                
                # AI prompt
                st.markdown("### ðŸ“ AI Prompt")
                prompt_text = st.text_area(
                    "AI Scoring Prompt",
                    value=ai_config.get("scoring_prompt", ""),
                    height=150,
                    help="Custom prompt for AI lead scoring"
                )
                if prompt_text:
                    CONFIG["ai_enrichment"]["scoring_prompt"] = prompt_text
            
            if st.button("Save AI Settings", type="primary"):
                try:
                    with open(CONFIG_FILE, "w") as f:
                        json.dump(CONFIG, f, indent=2)
                    st.success("AI settings saved!")
                except Exception as e:
                    st.error(f"Error: {e}")
        
        with tab6:
            st.subheader("ðŸ“Š CRM Settings")
            
            crm_config = CONFIG.get("crm", {})
            
            col1, col2 = st.columns(2)
            
            with col1:
                CONFIG["crm"]["enabled"] = st.toggle(
                    "Enable CRM",
                    value=crm_config.get("enabled", True)
                )
                CONFIG["crm"]["auto_sync"] = st.toggle(
                    "Auto Sync Leads",
                    value=crm_config.get("auto_sync", True)
                )
                CONFIG["crm"]["prevent_duplicates"] = st.toggle(
                    "Prevent Duplicates",
                    value=crm_config.get("prevent_duplicates", True)
                )
            
            with col2:
                status_options = CONFIG["lead_management"]["status_options"]
                default_status = crm_config.get("default_status", "New Lead")
                status_index = status_options.index(default_status) if default_status in status_options else 0
                
                CONFIG["crm"]["default_status"] = st.selectbox(
                    "Default Status",
                    options=status_options,
                    index=status_index
                )
                
                CONFIG["crm"]["default_assigned_to"] = st.text_input(
                    "Default Assigned To",
                    value=crm_config.get("default_assigned_to", "")
                )
                
                CONFIG["crm"]["auto_set_production_date"] = st.toggle(
                    "Auto Set Production Date",
                    value=crm_config.get("auto_set_production_date", True)
                )
            
            # Lead management settings
            st.markdown("### ðŸ‘¥ Lead Management")
            
            lm_config = CONFIG.get("lead_management", {})
            
            col1, col2 = st.columns(2)
            
            with col1:
                CONFIG["lead_management"]["default_follow_up_days"] = st.number_input(
                    "Default Follow-up Days",
                    value=lm_config.get("default_follow_up_days", 7),
                    min_value=1, max_value=30
                )
                CONFIG["lead_management"]["auto_archive_days"] = st.number_input(
                    "Auto Archive Days",
                    value=lm_config.get("auto_archive_days", 90),
                    min_value=1, max_value=365
                )
            
            with col2:
                CONFIG["lead_management"]["default_meeting_reminder_hours"] = st.number_input(
                    "Meeting Reminder Hours",
                    value=lm_config.get("default_meeting_reminder_hours", 24),
                    min_value=1, max_value=168
                )
            
            # Status options
            st.markdown("### ðŸ“‹ Status Options")
            status_text = st.text_area(
                "Status Options (one per line)",
                value="\n".join(lm_config.get("status_options", [])),
                height=150
            )
            if status_text:
                CONFIG["lead_management"]["status_options"] = [s.strip() for s in status_text.split("\n") if s.strip()]
            
            if st.button("Save CRM Settings", type="primary"):
                try:
                    with open(CONFIG_FILE, "w") as f:
                        json.dump(CONFIG, f, indent=2)
                    st.success("CRM settings saved!")
                except Exception as e:
                    st.error(f"Error: {e}")
    
    def render_logs(self):
        """Render logs page"""
        st.title("ðŸ“‹ System Logs")
        
        log_file = CONFIG["storage"]["logs_file"]
        
        if os.path.exists(log_file):
            try:
                with open(log_file, "r") as f:
                    logs = json.load(f)
                
                # Filter options
                col1, col2, col3 = st.columns(3)
                with col1:
                    level_filter = st.selectbox("Filter Level", ["All", "INFO", "SUCCESS", "WARNING", "ERROR", "DEBUG"])
                with col2:
                    search_term = st.text_input("Search logs")
                
                # Apply filters
                filtered_logs = logs
                
                if level_filter != "All":
                    filtered_logs = [log for log in filtered_logs if log.get("level") == level_filter]
                
                if search_term:
                    filtered_logs = [log for log in filtered_logs if search_term.lower() in log.get("message", "").lower()]
                
                # Display logs
                st.subheader(f"Log Entries ({len(filtered_logs)})")
                
                # Create log viewer
                if filtered_logs:
                    # Reverse to show newest first
                    filtered_logs.reverse()
                    
                    for log in filtered_logs[:100]:
                        timestamp = log.get("timestamp", "")[:19]
                        level = log.get("level", "INFO")
                        message = log.get("message", "")
                        
                        if level == "ERROR":
                            color = "#ef4444"
                            icon = "âŒ"
                        elif level == "WARNING":
                            color = "#f59e0b"
                            icon = "âš ï¸"
                        elif level == "SUCCESS":
                            color = "#10b981"
                            icon = "âœ…"
                        elif level == "DEBUG":
                            color = "#6b7280"
                            icon = "ðŸ”"
                        else:
                            color = "#3b82f6"
                            icon = "â„¹ï¸"
                        
                        st.markdown(f"""
                        <div style='border-left: 3px solid {color}; padding-left: 10px; margin: 5px 0;'>
                            <small style='color: #94a3b8'>{timestamp}</small><br/>
                            <strong style='color: {color}'>{icon} {level}:</strong> {message}
                        </div>
                        """, unsafe_allow_html=True)
                else:
                    st.info("No logs match the current filters")
                
                # Clear logs button
                if st.button("ðŸ—‘ï¸ Clear All Logs", type="secondary"):
                    try:
                        with open(log_file, "w") as f:
                            json.dump([], f)
                        st.success("Logs cleared!")
                        st.rerun()
                    except Exception as e:
                        st.error(f"Error: {e}")
                
            except Exception as e:
                st.error(f"Could not load logs: {e}")
        else:
            st.info("No logs available yet")
    
    def render_export(self):
        """Render export page"""
        st.title("ðŸ“¤ Export Data")
        
        # Export options
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Export Options")
            
            export_format = st.radio(
                "Export Format",
                ["CSV", "JSON", "Excel"],
                horizontal=True
            )
            
            # Column selection
            available_columns = [
                "business_name", "website", "phone", "email", "address",
                "city", "state", "industry", "business_type", "services",
                "lead_score", "quality_tier", "potential_value", "lead_status",
                "assigned_to", "created_at", "scraped_date", "has_website",
                "is_directory_listing", "directory_source", "running_google_ads",
                "rating", "review_count", "outreach_priority"
            ]
            
            selected_columns = st.multiselect(
                "Select Columns to Export",
                options=available_columns,
                default=["business_name", "phone", "email", "city", "lead_score", "quality_tier", "lead_status"]
            )
            
            # Date range
            st.subheader("Date Range")
            col_date1, col_date2 = st.columns(2)
            with col_date1:
                date_from = st.date_input("From Date", value=None)
            with col_date2:
                date_to = st.date_input("To Date", value=None)
        
        with col2:
            st.subheader("Filters")
            
            # Status filter
            status_filter = st.multiselect(
                "Status",
                options=CONFIG["lead_management"]["status_options"],
                default=[]
            )
            
            # Quality filter
            quality_filter = st.multiselect(
                "Quality Tier",
                options=CONFIG["lead_management"]["quality_tiers"],
                default=[]
            )
            
            # City filter
            city_filter = st.multiselect(
                "City",
                options=CONFIG["cities"],
                default=[]
            )
            
            # Additional filters
            website_filter = st.selectbox(
                "Has Website",
                options=["All", "Yes", "No"]
            )
            
            directory_filter = st.selectbox(
                "Directory Listing",
                options=["All", "Yes", "No"]
            )
            
            ads_filter = st.selectbox(
                "Running Ads",
                options=["All", "Yes", "No"]
            )
        
        # Build filters
        filters = {}
        if status_filter:
            filters["status"] = status_filter[0]
        if quality_filter:
            filters["quality_tier"] = quality_filter[0]
        if city_filter:
            filters["city"] = city_filter[0]
        if date_from:
            filters["date_from"] = date_from.isoformat()
        if date_to:
            filters["date_to"] = date_to.isoformat()
        if website_filter == "Yes":
            filters["has_website"] = 1
        elif website_filter == "No":
            filters["has_website"] = 0
        if directory_filter == "Yes":
            filters["is_directory"] = 1
        elif directory_filter == "No":
            filters["is_directory"] = 0
        if ads_filter == "Yes":
            filters["running_ads"] = 1
        elif ads_filter == "No":
            filters["running_ads"] = 0
        
        # Get filtered data
        leads_data = self.crm.get_leads(filters=filters, page=1, per_page=10000)
        leads = leads_data["leads"]
        
        st.metric("Leads to Export", len(leads))
        
        if leads:
            # Convert to DataFrame
            df = pd.DataFrame(leads)
            
            # Filter columns
            if selected_columns:
                available_cols = [col for col in selected_columns if col in df.columns]
                df = df[available_cols]
            
            # Preview
            st.subheader("Preview (First 10 Rows)")
            st.dataframe(df.head(10), use_container_width=True)
            
            # Export buttons
            st.subheader("Download")
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if export_format == "CSV":
                csv_data = df.to_csv(index=False)
                st.download_button(
                    label="ðŸ“¥ Download CSV",
                    data=csv_data,
                    file_name=f"leads_export_{timestamp}.csv",
                    mime="text/csv",
                    type="primary"
                )
            
            elif export_format == "JSON":
                json_data = df.to_json(orient="records", indent=2)
                st.download_button(
                    label="ðŸ“¥ Download JSON",
                    data=json_data,
                    file_name=f"leads_export_{timestamp}.json",
                    mime="application/json",
                    type="primary"
                )
            
            elif export_format == "Excel":
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='Leads')
                
                st.download_button(
                    label="ðŸ“¥ Download Excel",
                    data=buffer.getvalue(),
                    file_name=f"leads_export_{timestamp}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    type="primary"
                )
        else:
            st.warning("No leads to export with the current filters.")
    
    def run(self):
        """Run dashboard"""
        if not self.enabled:
            st.error("Dashboard not available")
            return
        
        # Initialize session state
        if 'scraper_running' not in st.session_state:
            st.session_state.scraper_running = False
        if 'scraper_stats' not in st.session_state:
            st.session_state.scraper_stats = {}
        if 'selected_lead_id' not in st.session_state:
            st.session_state.selected_lead_id = 1
        
        # Render
        page = self.render_sidebar()
        
        # Route to appropriate page
        if page == "Dashboard":
            self.render_dashboard()
        elif page == "Leads":
            self.render_leads()
        elif page == "Lead Details":
            self.render_lead_details()
        elif page == "Settings":
            self.render_settings()
        elif page == "Logs":
            self.render_logs()
        elif page == "Export":
            self.render_export()
        
        # Auto-refresh
        if st.session_state.scraper_running and CONFIG["dashboard"].get("auto_refresh", True):
            refresh_interval = CONFIG["dashboard"].get("refresh_interval", 30000)
            st_autorefresh(interval=refresh_interval, key="scraper_refresh")

# ============================================================================
# MAIN EXECUTION - 100% WORKING
# ============================================================================

def main():
    print("\n" + "="*80)
    print("ðŸš€ COMPLETE PRODUCTION LEAD SCRAPER CRM - 100% WORKING")
    print("="*80)
    print("Features:")
    print("  âœ… ALL pages: Dashboard, Leads, Lead Details, Settings, Logs, Export")
    print("  âœ… Complete targeting control: Businesses WITH and WITHOUT websites")
    print("  âœ… Directory scraping: Yelp, YellowPages, BBB, Chamber of Commerce")
    print("  âœ… Google Ads detection with Ads Transparency Center")
    print("  âœ… Google Business Profile extraction")
    print("  âœ… AI-powered lead qualification and scoring")
    print("  âœ… Complete CRM with all database columns")
    print("  âœ… Advanced filtering and search capabilities")
    print("  âœ… Multiple export formats (CSV, JSON, Excel)")
    print("  âœ… Real-time statistics and monitoring")
    print("  âœ… MitzMedia-inspired professional design")
    print("  âœ… ALL ERRORS FIXED - 100% WORKING")
    print("="*80)
    
    # Check API status
    serper_key = CONFIG.get("serper_api_key", "")
    openai_key = CONFIG.get("openai_api_key", "")
    
    if not serper_key or serper_key == "YOUR_SERPER_API_KEY":
        print("\nâŒ Serper API key not configured")
        print("   Get from: https://serper.dev")
        print("   Update in Settings â†’ API Keys")
    else:
        print("\nâœ… Serper API: Configured")
    
    if not openai_key or openai_key == "YOUR_OPENAI_API_KEY":
        print("âš ï¸  OpenAI API: Not configured (AI features disabled)")
    else:
        print("âœ… OpenAI API: Configured")
    
    # Show targeting settings
    filters = CONFIG.get("filters", {})
    enhanced = CONFIG.get("enhanced_features", {})
    
    print(f"\nðŸŽ¯ Targeting Settings:")
    print(f"   â€¢ Include no-website leads: {not filters.get('exclude_without_websites', True)}")
    print(f"   â€¢ Directory listings: {filters.get('include_directory_listings', True)}")
    print(f"   â€¢ Directory sources: {len(CONFIG.get('directory_sources', []))}")
    print(f"   â€¢ Google Ads check: {enhanced.get('check_google_ads', True)}")
    print(f"   â€¢ Google Business: {enhanced.get('find_google_business', True)}")
    print(f"   â€¢ AI enrichment: {CONFIG.get('ai_enrichment', {}).get('enabled', True)}")
    print(f"ðŸ™ï¸  State: {CONFIG.get('state', 'PA')}")
    print(f"ðŸ™ï¸  Cities: {len(CONFIG.get('cities', []))}")
    print(f"ðŸ­ Industries: {len(CONFIG.get('industries', []))}")
    print(f"â±ï¸  Interval: {CONFIG.get('cycle_interval', 300)}s")
    print("="*80)
    
    if not STREAMLIT_AVAILABLE:
        print("\nâŒ Streamlit dependencies not installed")
        print("   Install with: pip install streamlit pandas plotly streamlit-autorefresh")
        return
    
    print(f"\nðŸŒ Dashboard: http://localhost:{CONFIG.get('dashboard', {}).get('port', 8501)}")
    print("\nðŸ“Š Available Pages:")
    print("  â€¢ Dashboard - Real-time statistics and charts")
    print("  â€¢ Leads - Complete lead management with advanced filtering")
    print("  â€¢ Lead Details - Detailed lead view with activity timeline")
    print("  â€¢ Settings - FULL configuration editing (API, Targeting, Scraper, Business, AI, CRM)")
    print("  â€¢ Logs - System logs viewer with filtering")
    print("  â€¢ Export - Data export in CSV, JSON, Excel formats")
    print("="*80)
    
    # Run dashboard
    try:
        dashboard = StreamlitDashboard()
        dashboard.run()
    except Exception as e:
        print(f"\nâŒ Dashboard error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # Check requirements
    if not REQUESTS_AVAILABLE:
        print("âŒ Install requirements: pip install requests beautifulsoup4")
        sys.exit(1)
    
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nðŸ›‘ Stopped by user")
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
